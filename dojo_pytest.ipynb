{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65740e9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%cd ~/dojo-pytest/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577a8727",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dojo Pytest (Ou comment faire que des tests qui roxxent du poney)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887696a9",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Objectif : faire découvrir de manière peut-être un petit peu plus avancée le framework de test que nous utilisons.\n",
    "\n",
    "Sujet dense : ce ne sera pas exhaustif."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fa9b4a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "(Et RISE, c'est bien aussi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35b0f2d",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "RISE: Outil utilisé pour cette présentation. Basé sur Reveal.JS\n",
    "    \n",
    "https://rise.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e06cb5b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Avant de commencer :\n",
    "\n",
    "Versions utilisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4798e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python --version\n",
    "pytest --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fe74fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"bases\"></a>\n",
    "# Bases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b87a00",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exemple simple de test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cc6962",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile tests/bases/test_simple_example.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "def test_foo():\n",
    "    assert True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cac424",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "`%%writefile` -> commande magique (feature de IPython utilisable dans les notebooks Jupyter).\n",
    "\n",
    "Permet de faire autre-chose que de l'exécution de python par défaut.\n",
    "Autres exemples :\n",
    "- `%cd`\n",
    "- `%%script`\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "https://ipython.readthedocs.io/en/stable/interactive/magics.html\n",
    "\n",
    "\n",
    "Par défaut :\n",
    "- Infos utiles par défaut dans l'affichage\n",
    "- récapitulatif des tests + leurs résultats.\n",
    "\n",
    "\n",
    "`%%bash` -> Autre commande magique IPython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8471ebd2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pytest tests/bases/test_simple_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705a1f13",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Résultats possibles\n",
    "Il y en a principalement 3 :\n",
    "- Success.\n",
    "- Failure.\n",
    "- Error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf258cd",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Autres retours possibles :\n",
    "- `xfail`\n",
    "- `xpass`\n",
    "- `skip`\n",
    "\n",
    "Relativement anecdotiques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5c91fa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Dans le cas d'un succès (Success) :\n",
    "Pas d'exception non gérée, tout s'est bien passé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68374b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/bases/test_results_example_success.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "def test_success():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda0d68d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%script zsh --no-raise-error\n",
    "pytest tests/bases/test_results_example_success.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca21eac",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "À partir de maintenant, plus la magic command `%%bash`.\n",
    "(Retourne une erreur si la commande a un statut de retour autre que 0).\n",
    "\n",
    "`%%script bash --no-raise-error` permet d'éviter ce problème."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48d0f87",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Dans le cas d'un échec (Failure) :\n",
    "Une exception (de n'importe quel type) a été levée sans être gérée: le test n'est pas passé."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2da151",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Différence entre failure et error : source https://stackoverflow.com/a/32103555/3156085"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de963f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/bases/test_results_example_failure.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "def test_failure():\n",
    "    assert False, \"This test can only fail.\"\n",
    "\n",
    "def test_failure_alt():\n",
    "    raise AssertionError(\"This test can only fail.\")\n",
    "     \n",
    "def test_failure_other_exc():\n",
    "    raise ValueError(\"A test can fail with something else than an AssertionError.\")\n",
    "    \n",
    "def test_failure_with_division_error():\n",
    "    a = 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93429e67",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -v tests/bases/test_results_example_failure.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f02166",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Dans le cas d'une erreur (Error)\n",
    "Un problème est survenu _avant_ que le test n'ait été lancé (dans le setup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb87891",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/bases/test_results_example_error.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def foo():\n",
    "    return 1/0\n",
    "\n",
    "def test_error(foo):\n",
    "    \"\"\"This will issue an error because of ZeroDivisionError in fixture\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bdb2e4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest tests/bases/test_results_example_error.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c12d550",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Et si on attend la levée d'une exception ?\n",
    "\n",
    "On peut dans ce cas utiliser le manager de contexte [`pytest.raises`](https://docs.pytest.org/en/4.6.x/reference.html#pytest-raises) prévu spécialement pour ce genre de cas.\n",
    "\n",
    "Il prend en paramètre le type d'exception attendue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b4fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/bases/test_raises.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "def f_to_test():\n",
    "    d = {\"foo\": 42}\n",
    "    a = d[\"bar\"]  # This will raise a KeyError\n",
    "    \n",
    "def test_f_to_test():\n",
    "    with pytest.raises(KeyError):\n",
    "        f_to_test()\n",
    "\n",
    "def test_f_to_test_bis():\n",
    "    \"\"\"This test should fail.\"\"\"\n",
    "    with pytest.raises(ValueError):\n",
    "        f_to_test()\n",
    "        \n",
    "def test_without_exception():\n",
    "    \"\"\"This test should also fail.\"\"\"\n",
    "    with pytest.raises(ValueError):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898b73cd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -v tests/bases/test_raises.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b5d7cb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Conventions de nommage\n",
    "\n",
    "Pour qu'un test soit collecté par le comportement par défaut de pytest :\n",
    "- Il doit se trouver dans le package courant ou un de ses sous-packages.\n",
    "- Le module dans lequel il se trouve doit avoir son nom préfixé par \"test_\" (ou suffixé par \"_test.py\")\n",
    "- Le nom du test doit être préfixé par \"test\".\n",
    "- S'il se trouve dans une classe de test (sous forme de méthode), la classe doit avoir un nom préfixé par \"Test\" et la classe en question ne doit pas avoir de méthode `__init__()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b921954c",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Source: https://docs.pytest.org/en/7.1.x/explanation/goodpractices.html\n",
    "\n",
    "Tout ça reste aussi des comportements qui peuvent être modifiés par configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8170dbb8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exemple :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c396e64",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Le paramètre `--collect-only` sert à se contenter de collecter les tests. C'est-à-dire lister ceux qui sont disponibles.\n",
    "\n",
    "`pygmentize` est un équivalent de `cat` mais avec la coloration syntaxique de Python.\n",
    "https://pygments.org/docs/quickstart/\n",
    "\n",
    "Voici l'exemple d'une utilisation de pytest où on se contente de passer un dossier plutôt qu'un fichier spécifique. Il s'occupera alors de collecter tout seul les tests disponibles dans l'arborescence.\n",
    "\n",
    "Il y a aussi la possibilité de préciser un test particulier au sein d'un fichier de test à l'aide de l'opérateur `::`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8b4b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -x\n",
    "tree -I __pycache__ tests/bases/naming_conventions/\n",
    "pygmentize tests/bases/naming_conventions/test_collected_tests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898e43cf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest --collect-only tests/bases/naming_conventions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6741645e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Exemple plus complet\n",
    "\n",
    "Voici un exemple plus complet qui met en évidence les différences avec et sans `__init__.py`. (named packages vs. package IIRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0973d376",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -x\n",
    "tree -I __pycache__ tests/bases/naming_conventions_more_complete\n",
    "pygmentize tests/bases/naming_conventions_more_complete/subdir1/test_collected_tests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf062ce",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pytest --collect-only tests/bases/naming_conventions_more_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412a8210",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Note :\n",
    "Sans les `__init__.py` il semblerait que l'on puisse avoir quelque-chose qui ressemble à des erreurs d'import (collisions?).\n",
    "\n",
    "Comme [ici](https://stackoverflow.com/questions/53918088/import-file-mismatch-in-pytest).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462be6b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d29fe35",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Paramétrisation\n",
    "On veut parfois faire un même test pour plusieurs cas différents.\n",
    "Afin d'éviter la duplication, on peut simplement utiliser le même test avec plusieurs paramètres en entrée.\n",
    "\n",
    "Cela se déclare avec le décorateur [`pytest.mark.parametrize`](https://docs.pytest.org/en/6.2.x/parametrize.html#pytest-mark-parametrize-parametrizing-test-functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cca990",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af93284",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/parametrization/simple_parametrization/test_simple_parametrization_example.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.parametrize(\"mybool\", [False, True])\n",
    "def test_simple_parametrization_example(mybool):\n",
    "    print(\"\\nValue of mybool: %r\" % mybool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff23ae4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -s tests/concepts/parametrization/simple_parametrization/test_simple_parametrization_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d44928",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exemples avec un cas simple :\n",
    "#### Sans paramétrisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae62e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/parametrization/example_with_simple_case/without_parametrization.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "def is_even(i):\n",
    "    return (i % 2) == 0\n",
    "\n",
    "def test_is_even_1():\n",
    "    assert not is_even(1)\n",
    "\n",
    "def test_is_even_2():\n",
    "    assert is_even(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe1404",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -v tests/concepts/parametrization/example_with_simple_case/without_parametrization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1455e2dd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exemples avec un cas simple :\n",
    "#### Avec paramétrisation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434000b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/parametrization/example_with_simple_case/with_parametrization.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "def is_even(i):\n",
    "    return (i % 2) == 0\n",
    "\n",
    "@pytest.mark.parametrize(\"number,parity\", [(0, True), (1, False), (2, True)])\n",
    "def test_is_event(number, parity):\n",
    "    assert is_even(number) == parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ffb038",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -v tests/concepts/parametrization/example_with_simple_case/with_parametrization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a74ac6b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Truc : Chaînage de décorateurs\n",
    "Parfois, la paramétrisation peut être lourde à rédiger si les fonctions à tester ont beaucoup d'entrées possibles.\n",
    "\n",
    "La combinatoire peut donner beaucoup de cas !\n",
    "\n",
    "Chaîner les décorateurs parametrize peut donc considérablement alléger leur écriture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef5a1ea",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Exemples :\n",
    "##### Sans le chaînage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa50188",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/parametrization/chaining_trick/without_chaining.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.parametrize(\"bool1,bool2,bool3,bool4\", [\n",
    "    (False, False, False, False),\n",
    "    (False, False, False, True),\n",
    "    (False, False, True, False),\n",
    "    (False, False, True, True),\n",
    "    (False, True, False, False),\n",
    "    (False, True, False, True),\n",
    "    (False, True, True, False),\n",
    "    (False, True, True, True),\n",
    "    (True, False, False, False),\n",
    "    (True, False, False, True),\n",
    "    (True, False, True, False),\n",
    "    (True, False, True, True),\n",
    "    (True, True, False, False),\n",
    "    (True, True, False, True),\n",
    "    (True, True, True, False),\n",
    "    (True, True, True, True),\n",
    "])\n",
    "def test_complicated_function_with_many_inputs(bool1, bool2, bool3, bool4):\n",
    "    print(\"\\nbool1: %r - bool2: %r - bool3: %r - bool4: %r\" % (bool1, bool2, bool3, bool4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb13b90",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -v tests/concepts/parametrization/chaining_trick/without_chaining.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60e34eb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Examples :\n",
    "##### Avec le chaînage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/parametrization/chaining_trick/with_chaining.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.parametrize(\"bool1\", (False, True))\n",
    "@pytest.mark.parametrize(\"bool2\", (False, True))\n",
    "@pytest.mark.parametrize(\"bool3\", (False, True))\n",
    "@pytest.mark.parametrize(\"bool4\", (False, True))\n",
    "def test_complicated_function_with_many_inputs(bool1, bool2, bool3, bool4):\n",
    "    print(\"\\nbool1: %r - bool2: %r - bool3: %r - bool4: %r\" % (bool1, bool2, bool3, bool4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f536e24",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -v tests/concepts/parametrization/chaining_trick/with_chaining.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43b0866",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixtures\n",
    "\n",
    "L'une des fonctionalités clés de pytest est l'usage de fixtures. Les fixtures sont des fonctions décorées par [`@pytest.fixture`](https://docs.pytest.org/en/7.1.x/reference/reference.html#pytest-fixture-api). Elles permettent de préparer les tests, par exemple avec des sets de données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ebc77f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exemple et usage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3368209",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/fixtures/simple_example.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def myfixture():\n",
    "      return \"FOO\"\n",
    "\n",
    "def test_fixture(myfixture):\n",
    "    print(\"\\n%s\\n\" % myfixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88afd6d1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest tests/concepts/fixtures/simple_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8706d3ec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Une fixture peut faire appel à une autre fixture :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eafc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/fixtures/fixture_as_fixture_dependency.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def my_first_fixture():\n",
    "    return \"FOO\"\n",
    "\n",
    "@pytest.fixture\n",
    "def my_second_fixture(my_first_fixture):\n",
    "    return my_first_fixture * 2\n",
    "\n",
    "def test_fixture_as_fixture_dependency(my_second_fixture):\n",
    "    print(\"\\n%s\\n\" % my_second_fixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a608131",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -s tests/concepts/fixtures/fixture_as_fixture_dependency.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0785ba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Paramétrisation de fixture :\n",
    "\n",
    "Il est possible, tout comme pour les tests eux-mêmes, de paramétriser des tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b014a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/fixtures/fixtures_parametrisation.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture(params=[True, False])\n",
    "def parametrized_fixture(request):\n",
    "    return request.param\n",
    "\n",
    "def test_fixture_parametrization(parametrized_fixture):\n",
    "    print(\"\\n%r\\n\" % parametrized_fixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f93db13",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -s tests/concepts/fixtures/fixtures_parametrisation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8665061",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## NOTE :\n",
    "Question : peut-être devrait-on mettre au sein de la suite de test qui représenteraient des set de valeurs différentes (Utilisateurs avec droits avancés, booléens, Enums...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93ec131",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Problème possible : vouloir utiliser une fixture plusieurs fois avec paramétrisation pour un même test\n",
    "\n",
    "Quid si on a deux fixtures différentes qui veulent faire appel à une troisième, mais sans que l'on souhaite avoir les mêmes valeurs (dupliquer une même fixture)\n",
    "\n",
    "Le problème rencontré est alors que la fixture \"racine\" n'est pas dupliquée (la même valeur est utilisée à chaque fois et on ne pourrait pas par exemple pour une valeur booléenne faire cohabiter un `False` et un `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cc4fba",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/fixtures/fixtures_parametrisation_test_duplicate_direct.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture(params=[True, False])\n",
    "def boolean(request):\n",
    "    return request.param\n",
    "\n",
    "# This doesn't use fixture as fixtures but as local decorated functions.\n",
    "def test_duplicate_fixture(bool1=boolean, bool2=boolean):\n",
    "    print(\"%r - %r\\n\" % (bool1, bool2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289b9242",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -s tests/concepts/fixtures/fixtures_parametrisation_test_duplicate_direct.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236ce4ee",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/fixtures/fixtures_parametrisation_test_duplicate.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture(params=[True, False])\n",
    "def boolean(request):\n",
    "    return request.param\n",
    "\n",
    "@pytest.fixture\n",
    "def bool1(boolean):\n",
    "    return boolean\n",
    "\n",
    "@pytest.fixture\n",
    "def bool2(boolean):\n",
    "    return boolean\n",
    "\n",
    "# With this, each fixture (bool1, bool2) depends on the same fixture boolean. Only two cases.\n",
    "def test_duplicate_fixture(bool1, bool2):\n",
    "    print(\"%r - %r\\n\" % (bool1, bool2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2ca059",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest --collect-only tests/concepts/fixtures/fixtures_parametrisation_test_duplicate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f341cbd5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Workaround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc435335",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/fixtures/fixtures_parametrisation_test_workaround.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture(params=[True, False])\n",
    "def boolean(request):\n",
    "    return request.param\n",
    "\n",
    "# You have to make a copy of the fixture with a new name.\n",
    "boolean_bis = boolean\n",
    "\n",
    "def test_duplicate_fixture(boolean, boolean_bis):\n",
    "    print(\"\\n%r - %r\\n\" % (boolean, boolean_bis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83e3bfc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -s tests/concepts/fixtures/fixtures_parametrisation_test_workaround.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a741e41",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "## Fixtures builtin : (request, monkeypatch, mocker) [facultatif]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d5c066",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Conftest :\n",
    "\n",
    "Les fichiers conftest.py sont simplement les fichiers de configuration des suites de test.\n",
    "\n",
    "Typiquement les fichiers dans lesquels seront stockées les fixtures.\n",
    "\n",
    "Les fixtures seront accessibles par tous les tests (Et fixtures) dans l'arborescence du dossier dans lequel le `contest.py` se trouve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cef9acf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf824f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/fixtures/conftest_example/conftest.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def myfixture():\n",
    "    return __file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1434cf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/fixtures/conftest_example/test_mytest.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "def test_mytest(myfixture):\n",
    "    print(\"\\n%s\" % myfixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1419fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "mkdir tests/concepts/fixtures/conftest_example\n",
    "mkdir tests/concepts/fixtures/conftest_example/subdir1\n",
    "mkdir tests/concepts/fixtures/conftest_example/subdir2\n",
    "touch tests/concepts/fixtures/conftest_example/{__init__.py,subdir1/__init__.py,subdir2/__init__.py}\n",
    "cp tests/concepts/fixtures/conftest_example/{conftest.py,subdir1/}\n",
    "cp tests/concepts/fixtures/conftest_example/{test_mytest.py,subdir1/}\n",
    "cp tests/concepts/fixtures/conftest_example/{test_mytest.py,subdir2/}\n",
    "set -x\n",
    "tree tests/concepts/fixtures/conftest_example\n",
    "pytest -s tests/concepts/fixtures/conftest_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97b5d97",
   "metadata": {},
   "source": [
    "## Scope :\n",
    "La déclaration de fixture peut prendre en paramètre un paramètre `scope` inclus dans 5 valeurs :\n",
    "- `\"function\"` (valeur par défaut)\n",
    "- `\"class\"`\n",
    "- `\"module\"`\n",
    "- `\"package\"`\n",
    "- `\"session\"`\n",
    "\n",
    "Ce paramètre permet de réutiliser la même fixture au sein de plusieurs tests.\n",
    "\n",
    "Ainsi, une fixture avec le scope `\"session\"` sera invoquée une fois par session de test, une fois par package avec le scope `\"package\"`, etc.\n",
    "\n",
    "L'intérêt peut se trouver dans la recherche de meilleures performances. Avec des fixtures coûteuses en ressources dont l'invocation sera gérée plus finement, avec une mise en cache le reste du temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f29cfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/fixtures/scope/package1/conftest.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture(scope=\"function\")\n",
    "def function_fixture():\n",
    "    print(\"Function fixture called.\")\n",
    "    \n",
    "@pytest.fixture(scope=\"class\")\n",
    "def class_fixture():\n",
    "    print(\"Class fixture called.\")\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def module_fixture():\n",
    "    print(\"Module fixture called.\")\n",
    "\n",
    "@pytest.fixture(scope=\"package\")\n",
    "def package_fixture():\n",
    "    print(\"Package fixture called.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01069849",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/fixtures/scope/conftest.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def session_fixture():\n",
    "    print(\"Session fixture called.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba92dfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/fixtures/scope/package1/test_scope.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "class TestClass1:\n",
    "    def test_scope_1(self, function_fixture, class_fixture, module_fixture, package_fixture, session_fixture):\n",
    "        pass\n",
    "    \n",
    "    def test_scope_2(self, function_fixture, class_fixture, module_fixture, package_fixture, session_fixture):\n",
    "        pass\n",
    "\n",
    "class TestClass2:\n",
    "    def test_scope_1(self, function_fixture, class_fixture, module_fixture, package_fixture, session_fixture):\n",
    "        pass\n",
    "\n",
    "    def test_scope_2(self, function_fixture, class_fixture, module_fixture, package_fixture, session_fixture):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a469f84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "mkdir tests/concepts/fixtures/scope/package2\n",
    "cp tests/concepts/fixtures/scope/{package1/test_scope.py,package2/test_scope.py}\n",
    "cp tests/concepts/fixtures/scope/{package1/conftest.py,package2/conftest.py}\n",
    "touch tests/concepts/fixtures/scope/{__init__.py,package1/__init__.py,package2/__init__.py}\n",
    "\n",
    "pytest -s tests/concepts/fixtures/scope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206f1df4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mocking :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30449626",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Principe :\n",
    "\n",
    "Le mocking, dans le monde des test est le remplacement d'un élément par un autre avec des fonctionnalités utiles aux tests.\n",
    "\n",
    "C'est une pratique qui peut représenter plusieurs intérêts, parmi lesquelles :\n",
    "- Gain de performances pour l'exécution des tests.\n",
    "- Separation of concerns.\n",
    "- Introspection sur le comportement du code à des fins de test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6279a389",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Gain de performances pour l'exécution des tests :\n",
    "    Si une fonctionnalité à tester repose sur une méthode impliquant un calcul lourd et long, on pourra gagner du temps en mockant un objet renvoyant instantannément la valeur.\n",
    "\n",
    "## Separation of concerns :\n",
    "    On va sans doute vouloir aussi isoler au maximum le comportement de la feature testée. Si je veux tester une fonctionnalité A qui dépend de fonctionnalités B et C, le rôle de mon test sera de tester la fonctionnalité A sans que le résultat ne dépende des fonctionnalités B et C (On peut supposer que celles-ci sont cassées, pas encore codées...).\n",
    "\n",
    "## Introspection sur le comportement du code à des fins de test :\n",
    "    Cela peut être utile de s'assurer qu'une fonction a bien été appelée, avec tels paramètres..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f729e1d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `pytest-mock`/`unittest.mock` :\n",
    "\n",
    "`pytest-mock` est un paquet mettant à disposition de pytest un wrapper de la librairie `unittest.mock` de la librairie standard.\n",
    "\n",
    "Ce wrapper est accessible par la fixture `mocker` (qui la même api que `unittest.mock.patch`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dfb71a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Mocker une simple constante avec `mocker.path.object` :\n",
    "\n",
    "Voici un exemple simple pour commencer, on peut se contenter de mocker quelque-chose d'aussi simple qu'une constante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722b214",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/mocking/pytest-mock/simple_constant_mock/check_pwd_length.py\n",
    "#!/usr/bin/env python3\n",
    "PERMIT_SHORT_PASSWORDS = False\n",
    "\n",
    "def check_pwd_length(pwd):\n",
    "    print(f\"{PERMIT_SHORT_PASSWORDS!r}\")\n",
    "    if PERMIT_SHORT_PASSWORDS:\n",
    "        assert len(pwd) >= 8\n",
    "    else:\n",
    "        assert len(pwd) >= 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040f6d2c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[`mocker.patch.object`](https://docs.python.org/3.10/library/unittest.mock.html#patch-object) est ici utilisé avec 3 paramètres :\n",
    "- L'objet à patcher.\n",
    "- Le nom de l'attribut à patcher.\n",
    "- L'objet avec lequel remplacer l'attribut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f9f2a6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/mocking/pytest-mock/simple_constant_mock/test_simple_constant_mock.py\n",
    "import check_pwd_length\n",
    "\n",
    "def test_check_pwd_length_for_short_passwords(mocker):\n",
    "    mocker.patch.object(check_pwd_length, \"PERMIT_SHORT_PASSWORDS\", True)\n",
    "    check_pwd_length.check_pwd_length(\"petitpatapon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85991572",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -s tests/concepts/mocking/pytest-mock/simple_constant_mock/test_simple_constant_mock.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f366b55",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Note: On peut aussi utiliser `mocker.patch.object` avec deux arguments :\n",
    "\n",
    "On peut omettre l'objet avec lequel patcher la cible. Un mock en sera alors fait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a960e9a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/mocking/pytest-mock/mocker_patch_object_two_params/check_pwd_length.py\n",
    "#!/usr/bin/env python3\n",
    "PERMIT_SHORT_PASSWORDS = False\n",
    "\n",
    "\n",
    "def check_pwd_length(pwd):\n",
    "    print(f\"{PERMIT_SHORT_PASSWORDS!r}\")\n",
    "    if PERMIT_SHORT_PASSWORDS:\n",
    "        assert len(pwd) >= 8\n",
    "    else:\n",
    "        assert len(pwd) >= 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6f81ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/mocking/pytest-mock/mocker_patch_object_two_params/test_simple_constant_mock.py\n",
    "import check_pwd_length\n",
    "\n",
    "def test_check_pwd_length_for_short_passwords(mocker):\n",
    "    mock = mocker.patch.object(check_pwd_length, \"PERMIT_SHORT_PASSWORDS\")\n",
    "    check_pwd_length.check_pwd_length(\"petitpataponpluslong\")\n",
    "    mock.assert_not_called()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7e358e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -s tests/concepts/mocking/pytest-mock/mocker_patch_object_two_params/test_simple_constant_mock.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb27e6e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Attempt to merge both file from previous snippet.\n",
    "But requires self-referencing the current module as target.\n",
    "PEP 3130 would've allowed that but was rejected.\n",
    "https://peps.python.org/pep-3130/\n",
    "\"\"\"\n",
    "PERMIT_SHORT_PASSWORDS = False\n",
    "\n",
    "def check_pwd_length(pwd):\n",
    "    if PERMIT_SHORT_PASSWORDS:\n",
    "        assert len(pwd) >= 8\n",
    "    else:\n",
    "        assert len(pwd) >= 16\n",
    "\n",
    "def test_check_pwd_length_for_short_passwords(mocker):\n",
    "    with mocker.patch.object(__module__, \"PERMIT_SHORT_PASSWORDS\", True):\n",
    "        check_pwd_length(\"petitpatapon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab7d2d4",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Un peu limité avec une constante. :)\n",
    "\n",
    "Les methodes des objets `Mock` liées à l'examen de l'utilisation des fonctions n'a pas vraiment de sens ici.\n",
    "Mais il existe des classes de mock dédiées aux non-callables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd27b6eb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Mocker une fonction avec `mocker.patch` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043b0d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/mocking/pytest-mock/patching_a_function/answer.py\n",
    "#!/usr/bin/env python3\n",
    "from time import sleep\n",
    "\n",
    "def very_long_and_faulty_calculus():\n",
    "    sleep(5)\n",
    "    raise Exception(\"Oops\")\n",
    "    return 42\n",
    "\n",
    "def get_answer_to_everything():\n",
    "    return very_long_and_faulty_calculus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc15a13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/mocking/pytest-mock/patching_a_function/test_answer.py\n",
    "#!/usr/bin/env python3\n",
    "from answer import get_answer_to_everything\n",
    "\n",
    "def test_get_answer_to_everything_the_long_way():\n",
    "    assert get_answer_to_everything() == 42\n",
    "    \n",
    "def test_get_answer_to_everything(mocker):\n",
    "    mock = mocker.patch(\"answer.very_long_and_faulty_calculus\", return_value=42)\n",
    "    assert get_answer_to_everything() == 42\n",
    "    mock.assert_called()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8892d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -sv tests/concepts/mocking/pytest-mock/patching_a_function/test_answer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c1ae6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Utilisations en tant que décorateurs :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d36e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/mocking/pytest-mock/use_as_decorators/answer.py\n",
    "#!/usr/bin/env python3\n",
    "from time import sleep\n",
    "\n",
    "def calculus():\n",
    "    return 21\n",
    "\n",
    "def get_answer_to_everything():\n",
    "    return calculus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2127b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/mocking/pytest-mock/use_as_decorators/test_answers.py\n",
    "#!/usr/bin/env python3\n",
    "from answer import get_answer_to_everything\n",
    "from unittest.mock import patch\n",
    "import answer\n",
    "\n",
    "@patch(\"answer.calculus\", lambda: 42)\n",
    "def test_in_function():\n",
    "    assert get_answer_to_everything() == 42\n",
    "\n",
    "@patch.object(answer, \"calculus\", return_value=42)\n",
    "def test_in_function_bis(mock_method):\n",
    "    assert get_answer_to_everything() == 42\n",
    "    mock_method.assert_called_once()\n",
    "    \n",
    "@patch(\"answer.calculus\", lambda: 42)\n",
    "class TestInClass:\n",
    "    def test_answer(self):\n",
    "        assert get_answer_to_everything() == 42\n",
    "\n",
    "@patch.object(answer, \"calculus\", return_value=42)\n",
    "class TestInClassBis:\n",
    "    def test_answer(self, mock_method):\n",
    "        assert get_answer_to_everything() == 42\n",
    "        mock_method.assert_called_once()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f397b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -s tests/concepts/mocking/pytest-mock/use_as_decorators/test_answers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d79d7f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Les méthodes des objets Mock :\n",
    "\n",
    "Lors des tests, les objets de type `Mock` créés par les patchers mettent à disposition un snemble de méthodes afin de faire les tests.\n",
    "\n",
    "Les noms sont assez explicites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa4e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/mocking/pytest-mock/patching_a_function/test_answer.py\n",
    "#!/usr/bin/env python3\n",
    "from answer import get_answer_to_everything\n",
    "\n",
    "\n",
    "def test_get_answer_to_everything(mocker):\n",
    "    mock = mocker.patch(\"answer.very_long_and_faulty_calculus\", return_value=42)\n",
    "    mock.assert_not_called()\n",
    "    assert get_answer_to_everything() == 42\n",
    "    mock.assert_called_once()\n",
    "    mock.return_value = 43\n",
    "    assert get_answer_to_everything() == 43\n",
    "    mock.assert_called()\n",
    "    assert mock.return_value == 43\n",
    "    assert mock.call_count == 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d164643",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -sv tests/concepts/mocking/pytest-mock/patching_a_function/test_answer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379f646e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Monkeypatch\n",
    "\n",
    "[`monkeypatch`](https://docs.pytest.org/en/6.2.x/monkeypatch.html) est une fonctionnalité par défaut de pytest mise à disposition par la fixture built-in du même nom. Elle permet via un certain nombre de méthodes de gérer également les patchs dans les tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761fa627",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/mocking/monkeypatch/simple_constant_mock/check_pwd_length.py\n",
    "#!/usr/bin/env python3\n",
    "PERMIT_SHORT_PASSWORDS = False\n",
    "\n",
    "def check_pwd_length(pwd):\n",
    "    print(f\"{PERMIT_SHORT_PASSWORDS!r}\")\n",
    "    if PERMIT_SHORT_PASSWORDS:\n",
    "        assert len(pwd) >= 8\n",
    "    else:\n",
    "        assert len(pwd) >= 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77577610",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/mocking/monkeypatch/simple_constant_mock/test_simple_constant_mock.py\n",
    "#!/usr/bin/env python3\n",
    "import check_pwd_length\n",
    "\n",
    "def test_check_pwd_length_for_short_passwords(monkeypatch):\n",
    "    monkeypatch.setattr(\"check_pwd_length.PERMIT_SHORT_PASSWORDS\", True)\n",
    "    check_pwd_length.check_pwd_length(\"petitpatapon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a012c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest tests/concepts/mocking/monkeypatch/simple_constant_mock/test_simple_constant_mock.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef09db49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classes de test\n",
    "\n",
    "Les classes de test sont une manière alternative d'organiser les tests.\n",
    "Les tests sont alors les méthodes de cette classe préfixées par `test`. Comme dit en début de Dojo, ces classes ne doivent pas contenir de méthode `__init__()` et doivent avoir leur nom préfixé par `Test`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff381465",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exemple simple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16518f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/test_classes/simple_example.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "class TestClass:\n",
    "    def test_method(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db012754",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest tests/concepts/test_classes/simple_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68179339",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Paramétrisation de classes de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05d7c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/test_classes/parametrization.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.parametrize(\"mybool,mystr\", [(True, \"foo\"), (False, \"bar\")])\n",
    "class TestClass:\n",
    "    def test_mytest(self, mybool, mystr):\n",
    "        print(\"\\n%r, %r\" % (mybool, mystr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dbe583",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest tests/concepts/test_classes/parametrization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ed7876",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fixtures dans les classes de test\n",
    "\n",
    "On peut aussi définir une fixture dans le scope d'une classe de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc26f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/test_classes/test_fixture.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "class TestClass:\n",
    "    @pytest.fixture\n",
    "    def myfixt(self):\n",
    "        return 1\n",
    "    \n",
    "    def test_foo(self, myfixt):\n",
    "        assert myfixt == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc8300",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -v tests/concepts/test_classes/test_fixture.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef89c76c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Intérêt des classes de test\n",
    "\n",
    "Un intérêt majeur des classes de test se trouve dans l'utilisation de l'héritage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ebc179",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/test_classes/pros/test_example.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "class Bird:\n",
    "    can_fly = True\n",
    "    scream = \"Chirp!\"\n",
    "    \n",
    "    def get_scream(self):\n",
    "        return self.scream\n",
    "\n",
    "\n",
    "class Duck(Bird):\n",
    "    scream = \"Quack!\"\n",
    "\n",
    "\n",
    "class Swallow(Bird):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Penguin(Bird):\n",
    "    pass\n",
    "\n",
    "\n",
    "class TestBird:\n",
    "    bird_class = Bird\n",
    "    bird_scream = \"Chirp!\"\n",
    "    bird_flight_ability = True\n",
    "    \n",
    "    @pytest.fixture(scope=\"class\")\n",
    "    def thats_a_bird(self):\n",
    "        return self.bird_class()\n",
    "\n",
    "    def test_scream(self, thats_a_bird):\n",
    "        scream = thats_a_bird.get_scream()\n",
    "        expected_scream = self.bird_scream\n",
    "        assert scream == expected_scream, \"Scream for bird of type %r should be %r and not %r\" % (thats_a_bird.__class__, expected_scream, scream)\n",
    "\n",
    "    def test_flight_ability(self):\n",
    "        flight_ability = self.bird_class.can_fly\n",
    "        expected_flight_ability = self.bird_flight_ability\n",
    "        assert flight_ability == expected_flight_ability\n",
    "\n",
    "\n",
    "class TestDuck(TestBird):\n",
    "    bird_class = Duck\n",
    "    bird_scream = \"Quack!\"\n",
    "\n",
    "\n",
    "class TestSwallow(TestBird):\n",
    "    bird_class = Swallow\n",
    "\n",
    "\n",
    "class TestPenguin(TestBird):\n",
    "    bird_class = Penguin\n",
    "    bird_flight_ability = False\n",
    "    bird_scream = \"Je ne suis pas un pingouin.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589ae5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -v tests/concepts/test_classes/pros/test_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ab5b6c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Trucs :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2941d78e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Capture de l'outut avec le paramètre `-s`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d167e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/trucs/capture_of_output/test_foo.py\n",
    "def test_foo():\n",
    "     print(\"FOO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42afef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -s tests/trucs/capture_of_output/test_foo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c16f0e3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Se concentrer sur les fails avec `--last-failed` et `--failed-first`:\n",
    "\n",
    "Ces paramètres permettent respectivement de ne lancer que les tests ayant échoué et de les lancer avant ceux qui ont réussi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bf3bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/trucs/failed_tests/test_failed.py\n",
    "\n",
    "def test_success():\n",
    "    pass\n",
    "\n",
    "def test_failed():\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dd33f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -v tests/trucs/failed_tests/test_failed.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c99f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -v --last-failed tests/trucs/failed_tests/test_failed.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aec230",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -v --failed-first tests/trucs/failed_tests/test_failed.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db257c4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Continuer l'exécution des tests malgré les fails :\n",
    "\n",
    "Si je veux lancer tous les tests d'une suite de tests de 3500+ tests, et que je me doute que certains vont échouer, je peux tout-de-même faire tourner l'ensemble des tests avec `--maxfail=4000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e3e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/trucs/maxfail/test_maxfail.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "def test_fail_1():\n",
    "    assert False\n",
    "\n",
    "def test_success_1():\n",
    "    pass\n",
    "\n",
    "def test_fail_2():\n",
    "    assert False\n",
    "\n",
    "def test_success_2():\n",
    "    pass\n",
    "\n",
    "def test_fail_3():\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abce8e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest --maxfail=2 tests/trucs/maxfail/test_maxfail.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909fb361",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Se concentrer sur les tests frais avec `--new-first`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b3b110",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/trucs/newfirst/test_newfirst.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "def test_1():\n",
    "    pass\n",
    "\n",
    "def test_2():\n",
    "    pass\n",
    "\n",
    "def test_3():\n",
    "    pass\n",
    "\n",
    "# def test_4():\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0083b20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -v --new-first tests/trucs/newfirst/test_newfirst.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e9e21c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Le cache de pytest :\n",
    "Les derniers résultats sont stockés dans le cache de pytest. Pytest met à disposition deux outils pour l'inspecter et le vider.\n",
    "\n",
    "On peut ainsi l'inspecter avec `--cache-show` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3f548c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "cd tests/trucs/cache\n",
    "pytest --tb=no\n",
    "pytest --cache-show\n",
    "cd ~/dojo-pytest/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b638f",
   "metadata": {},
   "source": [
    "Et on peut le vider avec `--cache-clear` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7e0d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "cd tests/trucs/cache\n",
    "# --cache-clear re-runs the tests without --collect-only :)\n",
    "pytest --cache-clear --collect-only\n",
    "pytest --cache-show\n",
    "cd ~/dojo-pytest/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579b3bc0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MERCI !"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
