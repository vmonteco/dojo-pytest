{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda8cb44",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%cd ~/dojo-pytest/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c100156d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dojo Pytest (Ou comment faire des tests qui roxxent du poney)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56edddde",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Objectif : faire découvrir de manière peut-être un petit peu plus avancée le framework de test que nous utilisons.\n",
    "\n",
    "Sujet dense : ce ne sera pas exhaustif."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1bd070",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "(Et RISE, c'est bien aussi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c070511",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "RISE: Outil utilisé pour cette présentation. Basé sur Reveal.JS\n",
    "    \n",
    "https://rise.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e46e9cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Avant de commencer :\n",
    "\n",
    "Versions utilisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "c2588ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n",
      "pytest 7.4.4\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python --version\n",
    "pytest --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb88e6a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"bases\"></a>\n",
    "# Bases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc4a2cc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exemple simple de test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "e719b2e5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/bases/test_simple_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/bases/test_simple_example.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "def test_foo():\n",
    "    assert True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a550dd",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "`%%writefile` -> commande magique (feature de IPython utilisable dans les notebooks Jupyter).\n",
    "\n",
    "Permet de faire autre-chose que de l'exécution de python par défaut.\n",
    "Autres exemples :\n",
    "- `%cd`\n",
    "- `%%script`\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "https://ipython.readthedocs.io/en/stable/interactive/magics.html\n",
    "\n",
    "\n",
    "Par défaut :\n",
    "- Infos utiles par défaut dans l'affichage\n",
    "- récapitulatif des tests + leurs résultats.\n",
    "\n",
    "\n",
    "`%%bash` -> Autre commande magique IPython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "8825fa25",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "collected 1 item\n",
      "\n",
      "tests/bases/test_simple_example.py \u001b[32m.\u001b[0m\u001b[32m                                     [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pytest tests/bases/test_simple_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6850b1f2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Résultats possibles\n",
    "Il y en a principalement 3 :\n",
    "- Success.\n",
    "- Failure.\n",
    "- Error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca484db",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Autres retours possibles :\n",
    "- `xfail`\n",
    "- `xpass`\n",
    "- `skip`\n",
    "\n",
    "Relativement anecdotiques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ad8bcc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Dans le cas d'un succès (Success) :\n",
    "Pas d'exception non gérée, tout s'est bien passé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "f8313a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/bases/test_results_example_success.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/bases/test_results_example_success.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "def test_success():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "19791cf6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "collected 1 item\n",
      "\n",
      "tests/bases/test_results_example_success.py \u001b[32m.\u001b[0m\u001b[32m                            [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script zsh --no-raise-error\n",
    "pytest tests/bases/test_results_example_success.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cf8c62",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "À partir de maintenant, plus la magic command `%%bash`.\n",
    "(Retourne une erreur si la commande a un statut de retour autre que 0).\n",
    "\n",
    "`%%script bash --no-raise-error` permet d'éviter ce problème."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739cac6e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Dans le cas d'un échec (Failure) :\n",
    "Une exception (de n'importe quel type) a été levée sans être gérée: le test n'est pas passé."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa255c2d",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Différence entre failure et error : source https://stackoverflow.com/a/32103555/3156085"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "74944867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/bases/test_results_example_failure.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/bases/test_results_example_failure.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "def test_failure():\n",
    "    assert False, \"This test can only fail.\"\n",
    "\n",
    "def test_failure_alt():\n",
    "    raise AssertionError(\"This test can only fail.\")\n",
    "     \n",
    "def test_failure_other_exc():\n",
    "    raise ValueError(\"A test can fail with something else than an AssertionError.\")\n",
    "    \n",
    "def test_failure_with_division_error():\n",
    "    a = 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "01501ba9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0 -- /home/vmonteco/.envs/dojo-pytest/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 4 items\n",
      "\n",
      "tests/bases/test_results_example_failure.py::test_failure \u001b[31mFAILED\u001b[0m\u001b[31m         [ 25%]\u001b[0m\n",
      "tests/bases/test_results_example_failure.py::test_failure_alt \u001b[31mFAILED\u001b[0m\u001b[31m     [ 50%]\u001b[0m\n",
      "tests/bases/test_results_example_failure.py::test_failure_other_exc \u001b[31mFAILED\u001b[0m\u001b[31m [ 75%]\u001b[0m\n",
      "tests/bases/test_results_example_failure.py::test_failure_with_division_error \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________________ test_failure _________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_failure\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mThis test can only fail.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: This test can only fail.\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/bases/test_results_example_failure.py\u001b[0m:4: AssertionError\n",
      "\u001b[31m\u001b[1m_______________________________ test_failure_alt _______________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_failure_alt\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mAssertionError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mThis test can only fail.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: This test can only fail.\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/bases/test_results_example_failure.py\u001b[0m:7: AssertionError\n",
      "\u001b[31m\u001b[1m____________________________ test_failure_other_exc ____________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_failure_other_exc\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mA test can fail with something else than an AssertionError.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       ValueError: A test can fail with something else than an AssertionError.\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/bases/test_results_example_failure.py\u001b[0m:10: ValueError\n",
      "\u001b[31m\u001b[1m_______________________ test_failure_with_division_error _______________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_failure_with_division_error\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       a = \u001b[94m1\u001b[39;49;00m/\u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       ZeroDivisionError: division by zero\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/bases/test_results_example_failure.py\u001b[0m:13: ZeroDivisionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/bases/test_results_example_failure.py::\u001b[1mtest_failure\u001b[0m - AssertionError: This test can only fail.\n",
      "\u001b[31mFAILED\u001b[0m tests/bases/test_results_example_failure.py::\u001b[1mtest_failure_alt\u001b[0m - AssertionError: This test can only fail.\n",
      "\u001b[31mFAILED\u001b[0m tests/bases/test_results_example_failure.py::\u001b[1mtest_failure_other_exc\u001b[0m - ValueError: A test can fail with something else than an AssertionError.\n",
      "\u001b[31mFAILED\u001b[0m tests/bases/test_results_example_failure.py::\u001b[1mtest_failure_with_division_error\u001b[0m - ZeroDivisionError: division by zero\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m4 failed\u001b[0m\u001b[31m in 0.10s\u001b[0m\u001b[31m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -v tests/bases/test_results_example_failure.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e55667",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Dans le cas d'une erreur (Error)\n",
    "Un problème est survenu _avant_ que le test n'ait été lancé (dans le setup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "87a1f1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/bases/test_results_example_error.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/bases/test_results_example_error.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def foo():\n",
    "    return 1/0\n",
    "\n",
    "def test_error(foo):\n",
    "    \"\"\"This will issue an error because of ZeroDivisionError in fixture\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "37618c82",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "collected 1 item\n",
      "\n",
      "tests/bases/test_results_example_error.py \u001b[31mE\u001b[0m\u001b[31m                              [100%]\u001b[0m\n",
      "\n",
      "==================================== ERRORS ====================================\n",
      "\u001b[31m\u001b[1m_________________________ ERROR at setup of test_error _________________________\u001b[0m\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.fixture\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mfoo\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[94m1\u001b[39;49;00m/\u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       ZeroDivisionError: division by zero\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/bases/test_results_example_error.py\u001b[0m:6: ZeroDivisionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mERROR\u001b[0m tests/bases/test_results_example_error.py::\u001b[1mtest_error\u001b[0m - ZeroDivisionError: division by zero\n",
      "\u001b[31m=============================== \u001b[31m\u001b[1m1 error\u001b[0m\u001b[31m in 0.10s\u001b[0m\u001b[31m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest tests/bases/test_results_example_error.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d5a5b0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Et si on attend la levée d'une exception ?\n",
    "\n",
    "On peut dans ce cas utiliser le manager de contexte [`pytest.raises`](https://docs.pytest.org/en/4.6.x/reference.html#pytest-raises) prévu spécialement pour ce genre de cas.\n",
    "\n",
    "Il prend en paramètre le type d'exception attendue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "83b29a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/bases/test_raises.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/bases/test_raises.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "def f_to_test():\n",
    "    d = {\"foo\": 42}\n",
    "    a =  This will raise a KeyError\n",
    "    \n",
    "def test_f_to_test():\n",
    "    with pytest.raises(KeyError):\n",
    "        f_to_test()\n",
    "\n",
    "def test_f_to_test_bis():\n",
    "    \"\"\"This test should fail.\"\"\"\n",
    "    with pytest.raises(ValueError):\n",
    "        f_to_test()\n",
    "        \n",
    "def test_without_exception():\n",
    "    \"\"\"This test should also fail.\"\"\"\n",
    "    with pytest.raises(ValueError):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "631c2d8a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0 -- /home/vmonteco/.envs/dojo-pytest/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 0 items / 1 error\n",
      "\n",
      "==================================== ERRORS ====================================\n",
      "\u001b[31m\u001b[1m_________________ ERROR collecting tests/bases/test_raises.py __________________\u001b[0m\n",
      "\u001b[31m\u001b[1m\u001b[31m../.envs/dojo-pytest/lib/python3.10/site-packages/_pytest/python.py\u001b[0m:617: in _importtestmodule\n",
      "    mod = import_path(\u001b[96mself\u001b[39;49;00m.path, mode=importmode, root=\u001b[96mself\u001b[39;49;00m.config.rootpath)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../.envs/dojo-pytest/lib/python3.10/site-packages/_pytest/pathlib.py\u001b[0m:567: in import_path\n",
      "    importlib.import_module(module_name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/usr/lib/python3.10/importlib/__init__.py\u001b[0m:126: in import_module\n",
      "    \u001b[94mreturn\u001b[39;49;00m _bootstrap._gcd_import(name[level:], package, level)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:1050: in _gcd_import\n",
      "    \u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:1027: in _find_and_load\n",
      "    \u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:1006: in _find_and_load_unlocked\n",
      "    \u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m<frozen importlib._bootstrap>\u001b[0m:688: in _load_unlocked\n",
      "    \u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../.envs/dojo-pytest/lib/python3.10/site-packages/_pytest/assertion/rewrite.py\u001b[0m:177: in exec_module\n",
      "    source_stat, co = _rewrite_test(fn, \u001b[96mself\u001b[39;49;00m.config)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../.envs/dojo-pytest/lib/python3.10/site-packages/_pytest/assertion/rewrite.py\u001b[0m:359: in _rewrite_test\n",
      "    tree = ast.parse(source, filename=strfn)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/usr/lib/python3.10/ast.py\u001b[0m:50: in parse\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mcompile\u001b[39;49;00m(source, filename, mode, flags,\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE     File \"/home/vmonteco/dojo-pytest/tests/bases/test_raises.py\", line 6\u001b[0m\n",
      "\u001b[1m\u001b[31mE       a =  This will raise a KeyError\u001b[0m\n",
      "\u001b[1m\u001b[31mE                 ^^^^\u001b[0m\n",
      "\u001b[1m\u001b[31mE   SyntaxError: invalid syntax\u001b[0m\u001b[0m\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mERROR\u001b[0m tests/bases/test_raises.py\n",
      "!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n",
      "\u001b[31m=============================== \u001b[31m\u001b[1m1 error\u001b[0m\u001b[31m in 0.24s\u001b[0m\u001b[31m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -v tests/bases/test_raises.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7a171e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Conventions de nommage\n",
    "\n",
    "Pour qu'un test soit collecté par le comportement par défaut de pytest :\n",
    "- Il doit se trouver dans le package courant ou un de ses sous-packages.\n",
    "- Le module dans lequel il se trouve doit avoir son nom préfixé par \"test_\" (ou suffixé par \"_test.py\")\n",
    "- Le nom du test doit être préfixé par \"test\".\n",
    "- S'il se trouve dans une classe de test (sous forme de méthode), la classe doit avoir un nom préfixé par \"Test\" et la classe en question ne doit pas avoir de méthode `__init__()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ce971c",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Source: https://docs.pytest.org/en/7.1.x/explanation/goodpractices.html\n",
    "\n",
    "Tout ça reste aussi des comportements qui peuvent être modifiés par configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb93471",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exemple :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c046f129",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Le paramètre `--collect-only` sert à se contenter de collecter les tests. C'est-à-dire lister ceux qui sont disponibles.\n",
    "\n",
    "`pygmentize` est un équivalent de `cat` mais avec la coloration syntaxique de Python.\n",
    "https://pygments.org/docs/quickstart/\n",
    "\n",
    "Voici l'exemple d'une utilisation de pytest où on se contente de passer un dossier plutôt qu'un fichier spécifique. Il s'occupera alors de collecter tout seul les tests disponibles dans l'arborescence.\n",
    "\n",
    "Il y a aussi la possibilité de préciser un test particulier au sein d'un fichier de test à l'aide de l'opérateur `::`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "50b8e4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+ tree -I __pycache__ tests/bases/naming_conventions/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtests/bases/naming_conventions/\u001b[0m\n",
      "├── not_collected_tests.py\n",
      "└── test_collected_tests.py\n",
      "\n",
      "0 directories, 2 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+ pygmentize tests/bases/naming_conventions/test_collected_tests.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255mbasic_test\u001b[39m():\n",
      "    \u001b[38;2;0;128;0;01mpass\u001b[39;00m\n",
      "\n",
      "\n",
      "\u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255mtest_basic\u001b[39m():\n",
      "    \u001b[38;2;0;128;0;01mpass\u001b[39;00m\n",
      "\n",
      "\n",
      "\u001b[38;2;0;128;0;01mclass\u001b[39;00m \u001b[38;2;0;0;255;01mTestMyTestclass\u001b[39;00m:\n",
      "    \u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255mbasic_test\u001b[39m(\u001b[38;2;0;128;0mself\u001b[39m):\n",
      "        \u001b[38;2;0;128;0;01mpass\u001b[39;00m\n",
      "\n",
      "    \u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255mtest_basic\u001b[39m(\u001b[38;2;0;128;0mself\u001b[39m):\n",
      "        \u001b[38;2;0;128;0;01mpass\u001b[39;00m\n",
      "\n",
      "\n",
      "\u001b[38;2;0;128;0;01mclass\u001b[39;00m \u001b[38;2;0;0;255;01mMyTestclass\u001b[39;00m:\n",
      "    \u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255mbasic_test\u001b[39m(\u001b[38;2;0;128;0mself\u001b[39m):\n",
      "        \u001b[38;2;0;128;0;01mpass\u001b[39;00m\n",
      "\n",
      "    \u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255mtest_basic\u001b[39m(\u001b[38;2;0;128;0mself\u001b[39m):\n",
      "        \u001b[38;2;0;128;0;01mpass\u001b[39;00m\n",
      "\n",
      "\n",
      "\u001b[38;2;0;128;0;01mclass\u001b[39;00m \u001b[38;2;0;0;255;01mTestMyTestclassWithInit\u001b[39;00m:\n",
      "    \u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255m__init__\u001b[39m(\u001b[38;2;0;128;0mself\u001b[39m):\n",
      "        \u001b[38;2;0;128;0;01mpass\u001b[39;00m\n",
      "\n",
      "    \u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255mbasic_test\u001b[39m(\u001b[38;2;0;128;0mself\u001b[39m):\n",
      "        \u001b[38;2;0;128;0;01mpass\u001b[39;00m\n",
      "\n",
      "    \u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255mtest_basic\u001b[39m(\u001b[38;2;0;128;0mself\u001b[39m):\n",
      "        \u001b[38;2;0;128;0;01mpass\u001b[39;00m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -x\n",
    "tree -I __pycache__ tests/bases/naming_conventions/\n",
    "pygmentize tests/bases/naming_conventions/test_collected_tests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "9d1a491b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "collected 2 items\n",
      "\n",
      "<Module tests/bases/naming_conventions/test_collected_tests.py>\n",
      "  <Function test_basic>\n",
      "  <Class TestMyTestclass>\n",
      "    <Function test_basic>\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "tests/bases/naming_conventions/test_collected_tests.py:25\n",
      "  /home/vmonteco/dojo-pytest/tests/bases/naming_conventions/test_collected_tests.py:25: PytestCollectionWarning: cannot collect test class 'TestMyTestclassWithInit' because it has a __init__ constructor (from: tests/bases/naming_conventions/test_collected_tests.py)\n",
      "    class TestMyTestclassWithInit:\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[32m========================== \u001b[32m2 tests collected\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest --collect-only tests/bases/naming_conventions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e9de50",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Exemple plus complet\n",
    "\n",
    "Voici un exemple plus complet qui met en évidence les différences avec et sans `__init__.py`. (named packages vs. package IIRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3033659e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -x\n",
    "tree -I __pycache__ tests/bases/naming_conventions_more_complete\n",
    "pygmentize tests/bases/naming_conventions_more_complete/subdir1/test_collected_tests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0df51c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pytest --collect-only tests/bases/naming_conventions_more_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3f2011",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Note :\n",
    "Sans les `__init__.py` il semblerait que l'on puisse avoir quelque-chose qui ressemble à des erreurs d'import (collisions?).\n",
    "\n",
    "Comme [ici](https://stackoverflow.com/questions/53918088/import-file-mismatch-in-pytest).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9031a11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f304f2f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Paramétrisation\n",
    "On veut parfois faire un même test pour plusieurs cas différents.\n",
    "Afin d'éviter la duplication, on peut simplement utiliser le même test avec plusieurs paramètres en entrée.\n",
    "\n",
    "Cela se déclare avec le décorateur [`pytest.mark.parametrize`](https://docs.pytest.org/en/6.2.x/parametrize.html#pytest-mark-parametrize-parametrizing-test-functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13881e53",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "21a55986",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/parametrization/simple_parametrization/test_simple_parametrization_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/concepts/parametrization/simple_parametrization/test_simple_parametrization_example.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.parametrize(\"mybool\", [False, True])\n",
    "def test_simple_parametrization_example(mybool):\n",
    "    print(\"\\nValue of mybool: %r\" % mybool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267d958e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -s tests/concepts/parametrization/simple_parametrization/test_simple_parametrization_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0cd253",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exemples avec un cas simple :\n",
    "#### Sans paramétrisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "203887dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/parametrization/example_with_simple_case/without_parametrization.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/concepts/parametrization/example_with_simple_case/without_parametrization.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "def is_even(i):\n",
    "    return (i % 2) == 0\n",
    "\n",
    "def test_is_even_1():\n",
    "    assert not is_even(1)\n",
    "\n",
    "def test_is_even_2():\n",
    "    assert is_even(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "74e47303",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0 -- /home/vmonteco/.envs/dojo-pytest/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "tests/concepts/parametrization/example_with_simple_case/without_parametrization.py::test_is_even_1 \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/concepts/parametrization/example_with_simple_case/without_parametrization.py::test_is_even_2 \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -v tests/concepts/parametrization/example_with_simple_case/without_parametrization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b7ac7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exemples avec un cas simple :\n",
    "#### Avec paramétrisation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "c4eb0039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/parametrization/example_with_simple_case/with_parametrization.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/concepts/parametrization/example_with_simple_case/with_parametrization.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "def is_even(i):\n",
    "    return (i % 2) == 0\n",
    "\n",
    "@pytest.mark.parametrize(\"number,parity\", [(0, True) , (1, False), (2, True)])\n",
    "def test_is_event(number, parity):\n",
    "    assert is_even(number) == parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "46c29abc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0 -- /home/vmonteco/.envs/dojo-pytest/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 3 items\n",
      "\n",
      "tests/concepts/parametrization/example_with_simple_case/with_parametrization.py::test_is_event[0-True] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/concepts/parametrization/example_with_simple_case/with_parametrization.py::test_is_event[1-False] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tests/concepts/parametrization/example_with_simple_case/with_parametrization.py::test_is_event[2-True] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -v tests/concepts/parametrization/example_with_simple_case/with_parametrization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f806fae9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Truc : Chaînage de décorateurs\n",
    "Parfois, la paramétrisation peut être lourde à rédiger si les fonctions à tester ont beaucoup d'entrées possibles.\n",
    "\n",
    "La combinatoire peut donner beaucoup de cas !\n",
    "\n",
    "Chaîner les décorateurs parametrize peut donc considérablement alléger leur écriture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37de6912",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Exemples :\n",
    "##### Sans le chaînage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "8852f16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/parametrization/chaining_trick/without_chaining.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/concepts/parametrization/chaining_trick/without_chaining.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.parametrize(\"bool1,bool2,bool3,bool4\", [\n",
    "    (False, False, False, False),\n",
    "    (False, False, False, True),\n",
    "    (False, False, True, False),\n",
    "    (False, False, True, True),\n",
    "    (False, True, False, False),\n",
    "    (False, True, False, True),\n",
    "    (False, True, True, False),\n",
    "    (False, True, True, True),\n",
    "    (True, False, False, False),\n",
    "    (True, False, False, True),\n",
    "    (True, False, True, False),\n",
    "    (True, False, True, True),\n",
    "    (True, True, False, False),\n",
    "    (True, True, False, True),\n",
    "    (True, True, True, False),\n",
    "    (True, True, True, True),\n",
    "])\n",
    "def test_complicated_function_with_many_inputs(bool1, bool2, bool3, bool4):\n",
    "    print(\"\\nbool1: %r - bool2: %r - bool3: %r - bool4: %r\" % (bool1, bool2, bool3, bool4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "cd32993f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0 -- /home/vmonteco/.envs/dojo-pytest/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 16 items\n",
      "\n",
      "tests/concepts/parametrization/chaining_trick/without_chaining.py::test_complicated_function_with_many_inputs[False-False-False-False] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/without_chaining.py::test_complicated_function_with_many_inputs[False-False-False-True] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/without_chaining.py::test_complicated_function_with_many_inputs[False-False-True-False] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/without_chaining.py::test_complicated_function_with_many_inputs[False-False-True-True] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/without_chaining.py::test_complicated_function_with_many_inputs[False-True-False-False] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/without_chaining.py::test_complicated_function_with_many_inputs[False-True-False-True] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/without_chaining.py::test_complicated_function_with_many_inputs[False-True-True-False] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/without_chaining.py::test_complicated_function_with_many_inputs[False-True-True-True] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/without_chaining.py::test_complicated_function_with_many_inputs[True-False-False-False] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/without_chaining.py::test_complicated_function_with_many_inputs[True-False-False-True] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/without_chaining.py::test_complicated_function_with_many_inputs[True-False-True-False] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/without_chaining.py::test_complicated_function_with_many_inputs[True-False-True-True] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/without_chaining.py::test_complicated_function_with_many_inputs[True-True-False-False] \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/without_chaining.py::test_complicated_function_with_many_inputs[True-True-False-True] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/without_chaining.py::test_complicated_function_with_many_inputs[True-True-True-False] \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/without_chaining.py::test_complicated_function_with_many_inputs[True-True-True-True] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m16 passed\u001b[0m\u001b[32m in 0.05s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -v tests/concepts/parametrization/chaining_trick/without_chaining.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dca423",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Examples :\n",
    "##### Avec le chaînage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "50fbaaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/parametrization/chaining_trick/with_chaining.py\n"
     ]
    }
   ],
   "source": [
    " %%writefile tests/concepts/parametrization/chaining_trick/with_chaining.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.parametrize(\"bool1\", (False, True))\n",
    "@pytest.mark.parametrize(\"bool2\", (False, True))\n",
    "@pytest.mark.parametrize(\"bool3\", (False, True))\n",
    "@pytest.mark.parametrize(\"bool4\", (False, True))\n",
    "def test_complicated_function_with_many_inputs(bool1, bool2, bool3, bool4):\n",
    "    print(\"\\nbool1: %r - bool2: %r - bool3: %r - bool4: %r\" % (bool1, bool2, bool3, bool4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "c0b295a6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0 -- /home/vmonteco/.envs/dojo-pytest/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 16 items\n",
      "\n",
      "tests/concepts/parametrization/chaining_trick/with_chaining.py::test_complicated_function_with_many_inputs[False-False-False-False] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/with_chaining.py::test_complicated_function_with_many_inputs[False-False-False-True] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/with_chaining.py::test_complicated_function_with_many_inputs[False-False-True-False] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/with_chaining.py::test_complicated_function_with_many_inputs[False-False-True-True] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/with_chaining.py::test_complicated_function_with_many_inputs[False-True-False-False] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/with_chaining.py::test_complicated_function_with_many_inputs[False-True-False-True] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/with_chaining.py::test_complicated_function_with_many_inputs[False-True-True-False] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/with_chaining.py::test_complicated_function_with_many_inputs[False-True-True-True] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/with_chaining.py::test_complicated_function_with_many_inputs[True-False-False-False] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/with_chaining.py::test_complicated_function_with_many_inputs[True-False-False-True] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/with_chaining.py::test_complicated_function_with_many_inputs[True-False-True-False] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/with_chaining.py::test_complicated_function_with_many_inputs[True-False-True-True] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/with_chaining.py::test_complicated_function_with_many_inputs[True-True-False-False] \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/with_chaining.py::test_complicated_function_with_many_inputs[True-True-False-True] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/with_chaining.py::test_complicated_function_with_many_inputs[True-True-True-False] \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "tests/concepts/parametrization/chaining_trick/with_chaining.py::test_complicated_function_with_many_inputs[True-True-True-True] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m16 passed\u001b[0m\u001b[32m in 0.05s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -v tests/concepts/parametrization/chaining_trick/with_chaining.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7801c8e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixtures\n",
    "\n",
    "L'une des fonctionalités clés de pytest est l'usage de fixtures. Les fixtures sont des fonctions décorées par [`@pytest.fixture`](https://docs.pytest.org/en/7.1.x/reference/reference.html#pytest-fixture-api). Elles permettent de préparer les tests, par exemple avec des sets de données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a10003",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exemple et usage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "491b5fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/fixtures/simple_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/concepts/fixtures/simple_example.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def myfixture():\n",
    "      return \"FOO\"\n",
    "\n",
    "def test_fixture(myfixture):\n",
    "    print(\"\\n%s\\n\" % myfixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "02dc2cd2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "collected 1 item\n",
      "\n",
      "tests/concepts/fixtures/simple_example.py \u001b[32m.\u001b[0m\u001b[32m                              [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest tests/concepts/fixtures/simple_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b088519",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Une fixture peut faire appel à une autre fixture :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "bb3d5459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/fixtures/fixture_as_fixture_dependency.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/concepts/fixtures/fixture_as_fixture_dependency.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def my_first_fixture():\n",
    "    return \"FOO\"\n",
    "\n",
    "@pytest.fixture\n",
    "def my_second_fixture(my_first_fixture):\n",
    "    return my_first_fixture * 2\n",
    "\n",
    "def test_fixture_as_fixture_dependency(my_second_fixture):\n",
    "    print(\"\\n%s\\n\" % my_second_fixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "bcc79efb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "collected 1 item\n",
      "\n",
      "tests/concepts/fixtures/fixture_as_fixture_dependency.py \n",
      "FOOFOO\n",
      "\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -s tests/concepts/fixtures/fixture_as_fixture_dependency.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fb27be",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Paramétrisation de fixture :\n",
    "\n",
    "Il est possible, tout comme pour les tests eux-mêmes, de paramétriser des tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "dfa067de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/fixtures/fixtures_parametrisation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/concepts/fixtures/fixtures_parametrisation.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture(params=[True, False])\n",
    "def parametrized_fixture(request):\n",
    "    return request.param\n",
    "\n",
    "def test_fixture_parametrization(parametrized_fixture):\n",
    "    print(\"\\n%r\\n\" % parametrized_fixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "3c5acc43",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "collected 2 items\n",
      "\n",
      "tests/concepts/fixtures/fixtures_parametrisation.py \n",
      "True\n",
      "\n",
      "\u001b[32m.\u001b[0m\n",
      "False\n",
      "\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -s tests/concepts/fixtures/fixtures_parametrisation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d9322c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Problème possible : vouloir utiliser une fixture plusieurs fois avec paramétrisation pour un même test\n",
    "\n",
    "Quid si on a deux fixtures différentes qui veulent faire appel à une troisième, mais sans que l'on souhaite avoir les mêmes valeurs (dupliquer une même fixture)\n",
    "\n",
    "Le problème rencontré est alors que la fixture \"racine\" n'est pas dupliquée (la même valeur est utilisée à chaque fois et on ne pourrait pas par exemple pour une valeur booléenne faire cohabiter un `False` et un `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a175be3a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/fixtures/fixtures_parametrisation_test_duplicate_direct.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture(params=[True, False])\n",
    "def boolean(request):\n",
    "    return request.param\n",
    "\n",
    "# This doesn't use fixture as fixtures but as local decorated functions.\n",
    "def test_duplicate_fixture(bool1=boolean, bool2=boolean):\n",
    "    print(\"%r - %r\\n\" % (bool1, bool2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b263bff7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -s tests/concepts/fixtures/fixtures_parametrisation_test_duplicate_direct.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397de18d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/fixtures/fixtures_parametrisation_test_duplicate.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture(params=[True, False])\n",
    "def boolean(request):\n",
    "    return request.param\n",
    "\n",
    "@pytest.fixture\n",
    "def bool1(boolean):\n",
    "    return boolean\n",
    "\n",
    "@pytest.fixture\n",
    "def bool2(boolean):\n",
    "    return boolean\n",
    "\n",
    "# With this, each fixture (bool1, bool2) depends on the same fixture boolean. Only two cases.\n",
    "def test_duplicate_fixture(bool1, bool2):\n",
    "    print(\"%r - %r\\n\" % (bool1, bool2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3620a4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest --collect-only tests/concepts/fixtures/fixtures_parametrisation_test_duplicate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162ab7ac",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Workaround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5a5070",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile tests/concepts/fixtures/fixtures_parametrisation_test_workaround.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture(params=[True, False])\n",
    "def boolean(request):\n",
    "    return request.param\n",
    "\n",
    "# You have to make a copy of the fixture with a new name.\n",
    "boolean_bis = boolean\n",
    "\n",
    "def test_duplicate_fixture(boolean, boolean_bis):\n",
    "    print(\"\\n%r - %r\\n\" % (boolean, boolean_bis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5a269b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -s tests/concepts/fixtures/fixtures_parametrisation_test_workaround.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173d156a",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## NOTE :\n",
    "Question : peut-être devrait-on mettre au sein de la suite de test qui représenteraient des set de valeurs différentes (Utilisateurs avec droits avancés, booléens, Enums...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b0e172",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Conftest :\n",
    "\n",
    "Les fichiers conftest.py sont simplement les fichiers de configuration des suites de test.\n",
    "\n",
    "Typiquement les fichiers dans lesquels seront stockées les fixtures.\n",
    "\n",
    "Les fixtures seront accessibles par tous les tests (Et fixtures) dans l'arborescence du dossier dans lequel le `contest.py` se trouve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badff366",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "343dadc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/fixtures/conftest_example/conftest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/concepts/fixtures/conftest_example/conftest.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def myfixture():\n",
    "    return __file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "57d1d2fb",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/fixtures/conftest_example/test_mytest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/concepts/fixtures/conftest_example/test_mytest.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "def test_mytest(myfixture):\n",
    "    print(\"\\n%s\" % myfixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "71e0052a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘tests/concepts/fixtures/conftest_example’: File exists\n",
      "mkdir: cannot create directory ‘tests/concepts/fixtures/conftest_example/subdir1’: File exists\n",
      "mkdir: cannot create directory ‘tests/concepts/fixtures/conftest_example/subdir2’: File exists\n",
      "+ tree -I __pycache__ tests/concepts/fixtures/conftest_example\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtests/concepts/fixtures/conftest_example\u001b[0m\n",
      "├── __init__.py\n",
      "├── conftest.py\n",
      "├── \u001b[01;34msubdir1\u001b[0m\n",
      "│   ├── __init__.py\n",
      "│   ├── conftest.py\n",
      "│   └── test_mytest.py\n",
      "├── \u001b[01;34msubdir2\u001b[0m\n",
      "│   ├── __init__.py\n",
      "│   └── test_mytest.py\n",
      "└── test_mytest.py\n",
      "\n",
      "2 directories, 8 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+ pytest -sv tests/concepts/fixtures/conftest_example\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0 -- /home/vmonteco/.envs/dojo-pytest/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 3 items\n",
      "\n",
      "tests/concepts/fixtures/conftest_example/test_mytest.py::test_mytest \n",
      "/home/vmonteco/dojo-pytest/tests/concepts/fixtures/conftest_example/conftest.py\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "tests/concepts/fixtures/conftest_example/subdir1/test_mytest.py::test_mytest \n",
      "/home/vmonteco/dojo-pytest/tests/concepts/fixtures/conftest_example/subdir1/conftest.py\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "tests/concepts/fixtures/conftest_example/subdir2/test_mytest.py::test_mytest \n",
      "/home/vmonteco/dojo-pytest/tests/concepts/fixtures/conftest_example/conftest.py\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.04s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "mkdir tests/concepts/fixtures/conftest_example\n",
    "mkdir tests/concepts/fixtures/conftest_example/subdir1\n",
    "mkdir tests/concepts/fixtures/conftest_example/subdir2\n",
    "touch tests/concepts/fixtures/conftest_example/{__init__.py,subdir1/__init__.py,subdir2/__init__.py}\n",
    "cp tests/concepts/fixtures/conftest_example/{conftest.py,subdir1/}\n",
    "cp tests/concepts/fixtures/conftest_example/{test_mytest.py,subdir1/}\n",
    "cp tests/concepts/fixtures/conftest_example/{test_mytest.py,subdir2/}\n",
    "set -x\n",
    "tree -I __pycache__ tests/concepts/fixtures/conftest_example\n",
    "pytest -sv tests/concepts/fixtures/conftest_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faa1194",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Scope :\n",
    "La déclaration de fixture peut prendre en paramètre un paramètre `scope` inclus dans 5 valeurs :\n",
    "- `\"function\"` (valeur par défaut)\n",
    "- `\"class\"`\n",
    "- `\"module\"`\n",
    "- `\"package\"`\n",
    "- `\"session\"`\n",
    "\n",
    "Ce paramètre permet de réutiliser la même fixture au sein de plusieurs tests.\n",
    "\n",
    "Ainsi, une fixture avec le scope `\"session\"` sera invoquée une fois par session de test, une fois par package avec le scope `\"package\"`, etc.\n",
    "\n",
    "L'intérêt peut se trouver dans la recherche de meilleures performances. Avec des fixtures coûteuses en ressources dont l'invocation sera gérée plus finement, avec une mise en cache le reste du temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "2664f161",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/fixtures/scope/package1/conftest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/concepts/fixtures/scope/package1/conftest.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture(scope=\"function\")\n",
    "def function_fixture():\n",
    "    print(\"Function fixture called.\")\n",
    "    \n",
    "@pytest.fixture(scope=\"class\")\n",
    "def class_fixture():\n",
    "    print(\"Class fixture called.\")\n",
    "\n",
    "@pytest.fixture(scope=\"module\") \n",
    "def module_fixture():\n",
    "    print(\"Module fixture called.\")\n",
    "\n",
    "@pytest.fixture(scope=\"package\")\n",
    "def package_fixture():\n",
    "    print(\"Package fixture called.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "425ef14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/fixtures/scope/conftest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/concepts/fixtures/scope/conftest.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def session_fixture():\n",
    "    print(\"Session fixture called.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "1bad5158",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/fixtures/scope/package1/test_scope.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/concepts/fixtures/scope/package1/test_scope.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "class TestClass1:\n",
    "    def test_scope_1(self, function_fixture, class_fixture, module_fixture, package_fixture, session_fixture):\n",
    "        pass\n",
    "    \n",
    "    def test_scope_2(self, function_fixture, class_fixture, module_fixture, package_fixture, session_fixture):\n",
    "        pass\n",
    "\n",
    "class TestClass2:\n",
    "    def test_scope_1(self, function_fixture, class_fixture, module_fixture, package_fixture, session_fixture):\n",
    "        pass\n",
    "\n",
    "    def test_scope_2(self, function_fixture, class_fixture, module_fixture, package_fixture, session_fixture):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "4c6b5268",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘tests/concepts/fixtures/scope/package2’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "collected 8 items\n",
      "\n",
      "tests/concepts/fixtures/scope/package1/test_scope.py Session fixture called.\n",
      "Package fixture called.\n",
      "Module fixture called.\n",
      "Class fixture called.\n",
      "Function fixture called.\n",
      "\u001b[32m.\u001b[0mFunction fixture called.\n",
      "\u001b[32m.\u001b[0mClass fixture called.\n",
      "Function fixture called.\n",
      "\u001b[32m.\u001b[0mFunction fixture called.\n",
      "\u001b[32m.\u001b[0m\n",
      "tests/concepts/fixtures/scope/package2/test_scope.py Package fixture called.\n",
      "Module fixture called.\n",
      "Class fixture called.\n",
      "Function fixture called.\n",
      "\u001b[32m.\u001b[0mFunction fixture called.\n",
      "\u001b[32m.\u001b[0mClass fixture called.\n",
      "Function fixture called.\n",
      "\u001b[32m.\u001b[0mFunction fixture called.\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m8 passed\u001b[0m\u001b[32m in 0.05s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "mkdir tests/concepts/fixtures/scope/package2\n",
    "cp tests/concepts/fixtures/scope/{package1/test_scope.py,package2/test_scope.py}\n",
    "cp tests/concepts/fixtures/scope/{package1/conftest.py,package2/conftest.py}\n",
    "touch tests/concepts/fixtures/scope/{__init__.py,package1/__init__.py,package2/__init__.py}\n",
    "\n",
    "pytest -s tests/concepts/fixtures/scope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e52628a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mocking :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da117d2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Principe :\n",
    "\n",
    "Le mocking, dans le monde des test est le remplacement d'un élément par un autre avec des fonctionnalités utiles aux tests.\n",
    "\n",
    "C'est une pratique qui peut représenter plusieurs intérêts, parmi lesquelles :\n",
    "- Gain de performances pour l'exécution des tests.\n",
    "- Separation of concerns.\n",
    "- Introspection sur le comportement du code à des fins de test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0b3c32",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Gain de performances pour l'exécution des tests :\n",
    "    Si une fonctionnalité à tester repose sur une méthode impliquant un calcul lourd et long, on pourra gagner du temps en mockant un objet renvoyant instantannément la valeur.\n",
    "\n",
    "## Separation of concerns :\n",
    "    On va sans doute vouloir aussi isoler au maximum le comportement de la feature testée. Si je veux tester une fonctionnalité A qui dépend de fonctionnalités B et C, le rôle de mon test sera de tester la fonctionnalité A sans que le résultat ne dépende des fonctionnalités B et C (On peut supposer que celles-ci sont cassées, pas encore codées...).\n",
    "\n",
    "## Introspection sur le comportement du code à des fins de test :\n",
    "    Cela peut être utile de s'assurer qu'une fonction a bien été appelée, avec tels paramètres..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700e884e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `pytest-mock`/`unittest.mock` :\n",
    "\n",
    "`pytest-mock` est un paquet mettant à disposition de pytest un wrapper de la librairie `unittest.mock` de la librairie standard.\n",
    "\n",
    "Ce wrapper est accessible par la fixture `mocker` (qui la même api que `unittest.mock.patch`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab4b03a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Mocker une simple constante avec `mocker.path.object` :\n",
    "\n",
    "Voici un exemple simple pour commencer, on peut se contenter de mocker quelque-chose d'aussi simple qu'une constante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "d0ca77fd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/mocking/pytest-mock/simple_constant_mock/check_pwd_length.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/concepts/mocking/pytest-mock/simple_constant_mock/check_pwd_length.py\n",
    "#!/usr/bin/env python3\n",
    "PERMIT_SHORT_PASSWORDS = False\n",
    "\n",
    "def check_pwd_length(pwd):\n",
    "    print(f\"{PERMIT_SHORT_PASSWORDS!r}\")\n",
    "    if PERMIT_SHORT_PASSWORDS:\n",
    "        assert len(pwd) >= 8\n",
    "    else:\n",
    "        assert len(pwd) >= 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "1b3f54a3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/mocking/pytest-mock/simple_constant_mock/test_simple_constant_mock.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/concepts/mocking/pytest-mock/simple_constant_mock/test_simple_constant_mock.py\n",
    "import check_pwd_length\n",
    "\n",
    "def test_check_pwd_length_for_short_passwords(mocker):\n",
    "    mocker.patch.object(check_pwd_length, \"PERMIT_SHORT_PASSWORDS\", True)\n",
    "    check_pwd_length.check_pwd_length(\"petitpatapon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5879cb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[`mocker.patch.object`](https://docs.python.org/3.10/library/unittest.mock.html#patch-object) est ici utilisé avec 3 paramètres :\n",
    "- L'objet à patcher.\n",
    "- Le nom de l'attribut à patcher.\n",
    "- L'objet avec lequel remplacer l'attribut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "d4264fb8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "collected 1 item\n",
      "\n",
      "tests/concepts/mocking/pytest-mock/simple_constant_mock/test_simple_constant_mock.py True\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -s tests/concepts/mocking/pytest-mock/simple_constant_mock/test_simple_constant_mock.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5b4146",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Note: On peut aussi utiliser `mocker.patch.object` avec deux arguments :\n",
    "\n",
    "On peut omettre l'objet avec lequel patcher la cible. Un mock en sera alors fait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "eb4512b4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/mocking/pytest-mock/mocker_patch_object_two_params/check_pwd_length.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/concepts/mocking/pytest-mock/mocker_patch_object_two_params/check_pwd_length.py\n",
    "#!/usr/bin/env python3\n",
    "PERMIT_SHORT_PASSWORDS = False\n",
    "\n",
    "\n",
    "def check_pwd_length(pwd):\n",
    "    print(f\"{PERMIT_SHORT_PASSWORDS!r}\")\n",
    "    if PERMIT_SHORT_PASSWORDS:\n",
    "        assert len(pwd) >= 8\n",
    "    else:\n",
    "        assert len(pwd) >= 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "ecf820a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/mocking/pytest-mock/mocker_patch_object_two_params/test_simple_constant_mock.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/concepts/mocking/pytest-mock/mocker_patch_object_two_params/test_simple_constant_mock.py\n",
    "import check_pwd_length\n",
    "\n",
    "def test_check_pwd_length_for_short_passwords(mocker):\n",
    "    mock = mocker.patch.object(check_pwd_length, \"PERMIT_SHORT_PASSWORDS\")\n",
    "    check_pwd_length.check_pwd_length(\"petitpataponpluslong\")\n",
    "    mock.assert_not_called()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "26eb6e2f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "collected 1 item\n",
      "\n",
      "tests/concepts/mocking/pytest-mock/mocker_patch_object_two_params/test_simple_constant_mock.py <MagicMock name='PERMIT_SHORT_PASSWORDS' id='140388908180784'>\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -s tests/concepts/mocking/pytest-mock/mocker_patch_object_two_params/test_simple_constant_mock.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e3f210",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Attempt to merge both file from previous snippet.\n",
    "But requires self-referencing the current module as target.\n",
    "PEP 3130 would've allowed that but was rejected.\n",
    "https://peps.python.org/pep-3130/\n",
    "\"\"\"\n",
    "PERMIT_SHORT_PASSWORDS = False\n",
    "\n",
    "def check_pwd_length(pwd):\n",
    "    if PERMIT_SHORT_PASSWORDS:\n",
    "        assert len(pwd) >= 8\n",
    "    else:\n",
    "        assert len(pwd) >= 16\n",
    "\n",
    "def test_check_pwd_length_for_short_passwords(mocker):\n",
    "    with mocker.patch.object(__module__, \"PERMIT_SHORT_PASSWORDS\", True):\n",
    "        check_pwd_length(\"petitpatapon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8fbfb2",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Un peu limité avec une constante. :)\n",
    "\n",
    "Les methodes des objets `Mock` liées à l'examen de l'utilisation des fonctions n'a pas vraiment de sens ici.\n",
    "Mais il existe des classes de mock dédiées aux non-callables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7d3ad4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Mocker une fonction avec `mocker.patch` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "c6e3e8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/mocking/pytest-mock/patching_a_function/answer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/concepts/mocking/pytest-mock/patching_a_function/answer.py\n",
    "#!/usr/bin/env python3\n",
    "from time import sleep\n",
    "\n",
    "def very_long_and_faulty_calculus():\n",
    "    sleep(5)\n",
    "    raise Exception(\"Oops\")\n",
    "    return 42\n",
    "\n",
    "def get_answer_to_everything():\n",
    "    return very_long_and_faulty_calculus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "1b07af29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/mocking/pytest-mock/patching_a_function/test_answer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/concepts/mocking/pytest-mock/patching_a_function/test_answer.py\n",
    "#!/usr/bin/env python3\n",
    "from answer import get_answer_to_everything\n",
    "\n",
    "def test_get_answer_to_everything_the_long_way():\n",
    "    assert get_answer_to_everything() == 42\n",
    "    \n",
    "def test_get_answer_to_everything(mocker):\n",
    "    mock = mocker.patch(\"answer.very_long_and_faulty_calculus\", return_value=42)\n",
    "    assert get_answer_to_everything() == 42\n",
    "    mock.assert_called()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "e8cb6076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0 -- /home/vmonteco/.envs/dojo-pytest/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "tests/concepts/mocking/pytest-mock/patching_a_function/test_answer.py::test_get_answer_to_everything_the_long_way \u001b[31mFAILED\u001b[0m\n",
      "tests/concepts/mocking/pytest-mock/patching_a_function/test_answer.py::test_get_answer_to_everything \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m__________________ test_get_answer_to_everything_the_long_way __________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_get_answer_to_everything_the_long_way\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m get_answer_to_everything() == \u001b[94m42\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/concepts/mocking/pytest-mock/patching_a_function/test_answer.py\u001b[0m:5: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/concepts/mocking/pytest-mock/patching_a_function/answer.py\u001b[0m:10: in get_answer_to_everything\n",
      "    \u001b[94mreturn\u001b[39;49;00m very_long_and_faulty_calculus()\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mvery_long_and_faulty_calculus\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        sleep(\u001b[94m5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mException\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mOops\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       Exception: Oops\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/concepts/mocking/pytest-mock/patching_a_function/answer.py\u001b[0m:6: Exception\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/concepts/mocking/pytest-mock/patching_a_function/test_answer.py::\u001b[1mtest_get_answer_to_everything_the_long_way\u001b[0m - Exception: Oops\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 5.09s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -sv tests/concepts/mocking/pytest-mock/patching_a_function/test_answer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f78385",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Utilisations en tant que décorateurs :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "1541170c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/mocking/pytest-mock/use_as_decorators/answer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/concepts/mocking/pytest-mock/use_as_decorators/answer.py\n",
    "#!/usr/bin/env python3\n",
    "from time import sleep\n",
    "\n",
    "def calculus():\n",
    "    return 21\n",
    "\n",
    "def get_answer_to_everything():\n",
    "    return calculus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "41bc38fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/mocking/pytest-mock/use_as_decorators/test_answers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/concepts/mocking/pytest-mock/use_as_decorators/test_answers.py\n",
    "#!/usr/bin/env python3\n",
    "from answer import get_answer_to_everything\n",
    "from unittest.mock import patch\n",
    "import answer\n",
    "\n",
    "@patch(\"answer.calculus\", lambda: 42)\n",
    "def test_in_function():\n",
    "    assert get_answer_to_everything() == 42\n",
    "\n",
    "@patch.object(answer, \"calculus\", return_value=42)\n",
    "def test_in_function_bis(mock_method):\n",
    "    assert get_answer_to_everything() == 42\n",
    "    mock_method.assert_called_once()\n",
    "    \n",
    "@patch(\"answer.calculus\", lambda: 42)\n",
    "class TestInClass:\n",
    "    def test_answer(self):\n",
    "        assert get_answer_to_everything() == 42\n",
    "\n",
    "@patch.object(answer, \"calculus\", return_value=42)\n",
    "class TestInClassBis:\n",
    "    def test_answer(self, mock_method):\n",
    "        assert get_answer_to_everything() == 42\n",
    "        mock_method.assert_called_once()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "e86eaa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "collected 4 items\n",
      "\n",
      "tests/concepts/mocking/pytest-mock/use_as_decorators/test_answers.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -s tests/concepts/mocking/pytest-mock/use_as_decorators/test_answers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d7321c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Les méthodes des objets Mock :\n",
    "\n",
    "Lors des tests, les objets de type `Mock` créés par les patchers mettent à disposition un snemble de méthodes afin de faire les tests.\n",
    "\n",
    "Les noms sont assez explicites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "f969dc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/mocking/pytest-mock/patching_a_function/test_answer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/concepts/mocking/pytest-mock/patching_a_function/test_answer.py\n",
    "#!/usr/bin/env python3\n",
    "from answer import get_answer_to_everything\n",
    "\n",
    "\n",
    "def test_get_answer_to_everything(mocker):\n",
    "    mock = mocker.patch(\"answer.very_long_and_faulty_calculus\", return_value=42)\n",
    "    mock.assert_not_called()\n",
    "    assert get_answer_to_everything() == 42\n",
    "    mock.assert_called_once()\n",
    "    mock.return_value = 43\n",
    "    assert get_answer_to_everything() == 43\n",
    "    mock.assert_called()\n",
    "    assert mock.return_value == 43\n",
    "    assert mock.call_count == 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "fc0916aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0 -- /home/vmonteco/.envs/dojo-pytest/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "tests/concepts/mocking/pytest-mock/patching_a_function/test_answer.py::test_get_answer_to_everything \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -sv tests/concepts/mocking/pytest-mock/patching_a_function/test_answer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb97fd4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Monkeypatch\n",
    "\n",
    "[`monkeypatch`](https://docs.pytest.org/en/6.2.x/monkeypatch.html) est une fonctionnalité par défaut de pytest mise à disposition par la fixture built-in du même nom. Elle permet via un certain nombre de méthodes de gérer également les patchs dans les tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "c8eedd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/mocking/monkeypatch/simple_constant_mock/check_pwd_length.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/concepts/mocking/monkeypatch/simple_constant_mock/check_pwd_length.py\n",
    "#!/usr/bin/env python3\n",
    "PERMIT_SHORT_PASSWORDS = False\n",
    "\n",
    "def check_pwd_length(pwd):\n",
    "    print(f\"{PERMIT_SHORT_PASSWORDS!r}\")\n",
    "    if PERMIT_SHORT_PASSWORDS:\n",
    "        assert len(pwd) >= 8\n",
    "    else:\n",
    "        assert len(pwd) >= 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "7b33a90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/mocking/monkeypatch/simple_constant_mock/test_simple_constant_mock.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/concepts/mocking/monkeypatch/simple_constant_mock/test_simple_constant_mock.py\n",
    "#!/usr/bin/env python3\n",
    "import check_pwd_length\n",
    "\n",
    "def test_check_pwd_length_for_short_passwords(monkeypatch):\n",
    "    monkeypatch.setattr(\"check_pwd_length.PERMIT_SHORT_PASSWORDS\", True)\n",
    "    check_pwd_length.check_pwd_length(\"petitpatapon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "07b6972d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "collected 1 item\n",
      "\n",
      "tests/concepts/mocking/monkeypatch/simple_constant_mock/test_simple_constant_mock.py \u001b[32m.\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest tests/concepts/mocking/monkeypatch/simple_constant_mock/test_simple_constant_mock.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7bd958",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classes de test\n",
    "\n",
    "Les classes de test sont une manière alternative d'organiser les tests.\n",
    "Les tests sont alors les méthodes de cette classe préfixées par `test`. Comme dit en début de Dojo, ces classes ne doivent pas contenir de méthode `__init__()` et doivent avoir leur nom préfixé par `Test`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded9e30a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exemple simple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "7bb2d3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/test_classes/simple_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/concepts/test_classes/simple_example.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "class TestClass:\n",
    "    def test_method(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "9eb59b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "collected 1 item\n",
      "\n",
      "tests/concepts/test_classes/simple_example.py \u001b[32m.\u001b[0m\u001b[32m                          [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest tests/concepts/test_classes/simple_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18362c90",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Paramétrisation de classes de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "b98c2ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/test_classes/parametrization.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/concepts/test_classes/parametrization.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.parametrize(\"mybool,mystr\", [(True, \"foo\"), (False, \"bar\")])\n",
    "class TestClass:\n",
    "    def test_mytest(self, mybool, mystr):\n",
    "        print(\"\\n%r, %r\" % (mybool, mystr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "cbda11cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0 -- /home/vmonteco/.envs/dojo-pytest/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "tests/concepts/test_classes/parametrization.py::TestClass::test_mytest[True-foo] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/concepts/test_classes/parametrization.py::TestClass::test_mytest[False-bar] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -v tests/concepts/test_classes/parametrization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a351e88",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fixtures dans les classes de test\n",
    "\n",
    "On peut aussi définir une fixture dans le scope d'une classe de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "4839c6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/test_classes/test_fixture.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/concepts/test_classes/test_fixture.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "class TestClass:\n",
    "    @pytest.fixture\n",
    "    def myfixt(self):\n",
    "        return 1\n",
    "    \n",
    "    def test_foo(self, myfixt):\n",
    "        assert myfixt == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "4ae327ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0 -- /home/vmonteco/.envs/dojo-pytest/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "tests/concepts/test_classes/test_fixture.py::TestClass::test_foo \u001b[32mPASSED\u001b[0m\u001b[32m  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -v tests/concepts/test_classes/test_fixture.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc30a5e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Intérêt des classes de test\n",
    "\n",
    "Un intérêt majeur des classes de test se trouve dans l'utilisation de l'héritage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "290f5007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/concepts/test_classes/pros/test_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/concepts/test_classes/pros/test_example.py\n",
    "#!/usr/bin/env python3\n",
    "import pytest\n",
    "\n",
    "class Bird:\n",
    "    can_fly = True\n",
    "    scream = \"Chirp!\"\n",
    "    \n",
    "    def get_scream(self):\n",
    "        return self.scream\n",
    "\n",
    "\n",
    "class Duck(Bird):\n",
    "    scream = \"Quack!\"\n",
    "\n",
    "\n",
    "class Swallow(Bird):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Penguin(Bird):\n",
    "    can_fly = False\n",
    "    scream = \"Je ne suis pas un pingouin.\"\n",
    "\n",
    "\n",
    "class TestBird:\n",
    "    bird_class = Bird\n",
    "    bird_scream = \"Chirp!\"\n",
    "    bird_flight_ability = True\n",
    "    \n",
    "    @pytest.fixture(scope=\"class\")\n",
    "    def thats_a_bird(self):\n",
    "        return self.bird_class()\n",
    "\n",
    "    def test_scream(self, thats_a_bird):\n",
    "        scream = thats_a_bird.get_scream()\n",
    "        expected_scream = self.bird_scream\n",
    "        assert scream == expected_scream, \"Scream for bird of type %r should be %r and not %r\" % (thats_a_bird.__class__, expected_scream, scream)\n",
    "\n",
    "    def test_flight_ability(self):\n",
    "        flight_ability = self.bird_class.can_fly\n",
    "        expected_flight_ability = self.bird_flight_ability\n",
    "        assert flight_ability == expected_flight_ability\n",
    "\n",
    "\n",
    "class TestDuck(TestBird):\n",
    "    bird_class = Duck\n",
    "    bird_scream = \"Quack!\"\n",
    "\n",
    "\n",
    "class TestSwallow(TestBird):\n",
    "    bird_class = Swallow\n",
    "\n",
    "\n",
    "class TestPenguin(TestBird):\n",
    "    bird_class = Penguin\n",
    "    bird_flight_ability = False\n",
    "    bird_scream = \"Je ne suis pas un pingouin.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "db2c62db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0 -- /home/vmonteco/.envs/dojo-pytest/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 8 items\n",
      "\n",
      "tests/concepts/test_classes/pros/test_example.py::TestBird::test_scream \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/concepts/test_classes/pros/test_example.py::TestBird::test_flight_ability \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/concepts/test_classes/pros/test_example.py::TestDuck::test_scream \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/concepts/test_classes/pros/test_example.py::TestDuck::test_flight_ability \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/concepts/test_classes/pros/test_example.py::TestSwallow::test_scream \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/concepts/test_classes/pros/test_example.py::TestSwallow::test_flight_ability \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/concepts/test_classes/pros/test_example.py::TestPenguin::test_scream \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/concepts/test_classes/pros/test_example.py::TestPenguin::test_flight_ability \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m8 passed\u001b[0m\u001b[32m in 0.08s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -v tests/concepts/test_classes/pros/test_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce499b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Trucs :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a7b615",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Capture de l'outut avec le paramètre `-s`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "a34e3c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/trucs/capture_of_output/test_foo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/trucs/capture_of_output/test_foo.py\n",
    "def test_foo():\n",
    "     print(\"FOO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "c4b5fdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "collected 1 item\n",
      "\n",
      "tests/trucs/capture_of_output/test_foo.py FOO\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -s tests/trucs/capture_of_output/test_foo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e100ae43",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Se concentrer sur les fails avec `--last-failed` et `--failed-first`:\n",
    "\n",
    "Ces paramètres permettent respectivement de ne lancer que les tests ayant échoué et de les lancer avant ceux qui ont réussi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "159a7b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/trucs/failed_tests/test_failed.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/trucs/failed_tests/test_failed.py\n",
    "\n",
    "def test_success():\n",
    "    pass\n",
    "\n",
    "def test_failed():\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "e4e33bf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0 -- /home/vmonteco/.envs/dojo-pytest/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "tests/trucs/failed_tests/test_failed.py::test_success \u001b[32mPASSED\u001b[0m\u001b[32m             [ 50%]\u001b[0m\n",
      "tests/trucs/failed_tests/test_failed.py::test_failed \u001b[31mFAILED\u001b[0m\u001b[31m              [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________________ test_failed __________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_failed\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/trucs/failed_tests/test_failed.py\u001b[0m:6: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/trucs/failed_tests/test_failed.py::\u001b[1mtest_failed\u001b[0m - assert False\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.07s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -v tests/trucs/failed_tests/test_failed.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "2424b779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0 -- /home/vmonteco/.envs/dojo-pytest/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items / 1 deselected / 1 selected\n",
      "run-last-failure: rerun previous 1 failure\n",
      "\n",
      "tests/trucs/failed_tests/test_failed.py::test_failed \u001b[31mFAILED\u001b[0m\u001b[31m              [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________________ test_failed __________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_failed\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/trucs/failed_tests/test_failed.py\u001b[0m:6: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/trucs/failed_tests/test_failed.py::\u001b[1mtest_failed\u001b[0m - assert False\n",
      "\u001b[31m======================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m1 deselected\u001b[0m\u001b[31m in 0.09s\u001b[0m\u001b[31m ========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -v --last-failed tests/trucs/failed_tests/test_failed.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "8f42fdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0 -- /home/vmonteco/.envs/dojo-pytest/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "run-last-failure: rerun previous 1 failure first\n",
      "\n",
      "tests/trucs/failed_tests/test_failed.py::test_failed \u001b[31mFAILED\u001b[0m\u001b[31m              [ 50%]\u001b[0m\n",
      "tests/trucs/failed_tests/test_failed.py::test_success \u001b[32mPASSED\u001b[0m\u001b[31m             [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________________ test_failed __________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_failed\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/trucs/failed_tests/test_failed.py\u001b[0m:6: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/trucs/failed_tests/test_failed.py::\u001b[1mtest_failed\u001b[0m - assert False\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.08s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -v --failed-first tests/trucs/failed_tests/test_failed.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c6976b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Continuer l'exécution des tests malgré les fails :\n",
    "\n",
    "Si je veux lancer tous les tests d'une suite de tests de 3500+ tests, et que je me doute que certains vont échouer, je peux tout-de-même faire tourner l'ensemble des tests avec `--maxfail=4000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "c5285f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/trucs/maxfail/test_maxfail.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/trucs/maxfail/test_maxfail.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "def test_fail_1():\n",
    "    assert False\n",
    "\n",
    "def test_success_1():\n",
    "    pass\n",
    "\n",
    "def test_fail_2():\n",
    "    assert False\n",
    "\n",
    "def test_success_2():\n",
    "    pass\n",
    "\n",
    "def test_fail_3():\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "e87ac6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "collected 5 items\n",
      "\n",
      "tests/trucs/maxfail/test_maxfail.py \u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________________ test_fail_1 __________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_fail_1\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/trucs/maxfail/test_maxfail.py\u001b[0m:4: AssertionError\n",
      "\u001b[31m\u001b[1m_________________________________ test_fail_2 __________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_fail_2\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/trucs/maxfail/test_maxfail.py\u001b[0m:10: AssertionError\n",
      "\u001b[31m\u001b[1m_________________________________ test_fail_3 __________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_fail_3\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/trucs/maxfail/test_maxfail.py\u001b[0m:16: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/trucs/maxfail/test_maxfail.py::\u001b[1mtest_fail_1\u001b[0m - assert False\n",
      "\u001b[31mFAILED\u001b[0m tests/trucs/maxfail/test_maxfail.py::\u001b[1mtest_fail_2\u001b[0m - assert False\n",
      "\u001b[31mFAILED\u001b[0m tests/trucs/maxfail/test_maxfail.py::\u001b[1mtest_fail_3\u001b[0m - assert False\n",
      "\u001b[31m!!!!!!!!!!!!!!!!!!!!!!!!!! stopping after 3 failures !!!!!!!!!!!!!!!!!!!!!!!!!!!\u001b[0m\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m3 failed\u001b[0m, \u001b[32m2 passed\u001b[0m\u001b[31m in 0.10s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest --maxfail=3 tests/trucs/maxfail/test_maxfail.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ddd09f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Se concentrer sur les tests frais avec `--new-first`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "040b4b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/trucs/newfirst/test_newfirst.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/trucs/newfirst/test_newfirst.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "def test_1():\n",
    "    pass\n",
    "\n",
    "def test_2():\n",
    "    pass\n",
    "\n",
    "def test_3():\n",
    "    pass\n",
    "\n",
    "def test_4():\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "93d9c955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0 -- /home/vmonteco/.envs/dojo-pytest/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/vmonteco/dojo-pytest\n",
      "plugins: mock-3.12.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 4 items\n",
      "\n",
      "tests/trucs/newfirst/test_newfirst.py::test_1 \u001b[32mPASSED\u001b[0m\u001b[32m                     [ 25%]\u001b[0m\n",
      "tests/trucs/newfirst/test_newfirst.py::test_2 \u001b[32mPASSED\u001b[0m\u001b[32m                     [ 50%]\u001b[0m\n",
      "tests/trucs/newfirst/test_newfirst.py::test_3 \u001b[32mPASSED\u001b[0m\u001b[32m                     [ 75%]\u001b[0m\n",
      "tests/trucs/newfirst/test_newfirst.py::test_4 \u001b[31mFAILED\u001b[0m\u001b[31m                     [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m____________________________________ test_4 ____________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_4\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/trucs/newfirst/test_newfirst.py\u001b[0m:13: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/trucs/newfirst/test_newfirst.py::\u001b[1mtest_4\u001b[0m - assert False\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m3 passed\u001b[0m\u001b[31m in 0.10s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "pytest -v --new-first tests/trucs/newfirst/test_newfirst.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d561e78",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Le cache de pytest :\n",
    "Les derniers résultats sont stockés dans le cache de pytest. Pytest met à disposition deux outils pour l'inspecter et le vider.\n",
    "\n",
    "On peut ainsi l'inspecter avec `--cache-show` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "b183c3b3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0\n",
      "rootdir: /home/vmonteco/dojo-pytest/tests/trucs/cache\n",
      "plugins: mock-3.12.0\n",
      "collected 6 items\n",
      "\n",
      "test_foo.py \u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                           [ 33%]\u001b[0m\n",
      "subdir1/test_foo.py \u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                   [ 66%]\u001b[0m\n",
      "subdir2/test_foo.py \u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                   [100%]\u001b[0m\n",
      "\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m test_foo.py::\u001b[1mtest_fail\u001b[0m - assert False\n",
      "\u001b[31mFAILED\u001b[0m subdir1/test_foo.py::\u001b[1mtest_fail\u001b[0m - assert False\n",
      "\u001b[31mFAILED\u001b[0m subdir2/test_foo.py::\u001b[1mtest_fail\u001b[0m - assert False\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m3 failed\u001b[0m, \u001b[32m3 passed\u001b[0m\u001b[31m in 0.03s\u001b[0m\u001b[31m ==========================\u001b[0m\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0\n",
      "rootdir: /home/vmonteco/dojo-pytest/tests/trucs/cache\n",
      "plugins: mock-3.12.0\n",
      "cachedir: /home/vmonteco/dojo-pytest/tests/trucs/cache/.pytest_cache\n",
      "----------------------------- cache values for '*' -----------------------------\n",
      "cache/lastfailed contains:\n",
      "  {'subdir1/test_foo.py::test_fail': True,\n",
      "   'subdir2/test_foo.py::test_fail': True,\n",
      "   'test_foo.py::test_fail': True}\n",
      "cache/nodeids contains:\n",
      "  ['subdir1/test_foo.py::test_fail',\n",
      "   'subdir1/test_foo.py::test_success',\n",
      "   'subdir2/test_foo.py::test_fail',\n",
      "   'subdir2/test_foo.py::test_success',\n",
      "   'test_foo.py::test_fail',\n",
      "   'test_foo.py::test_success']\n",
      "cache/stepwise contains:\n",
      "  []\n",
      "\n",
      "\u001b[33m============================ \u001b[33mno tests ran\u001b[0m\u001b[33m in 0.00s\u001b[0m\u001b[33m =============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "cd tests/trucs/cache\n",
    "pytest --tb=no\n",
    "pytest --cache-show\n",
    "cd ~/dojo-pytest/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d712506f",
   "metadata": {},
   "source": [
    "Et on peut le vider avec `--cache-clear` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "e2dc956c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0\n",
      "rootdir: /home/vmonteco/dojo-pytest/tests/trucs/cache\n",
      "plugins: mock-3.12.0\n",
      "collected 6 items\n",
      "\n",
      "<Package cache>\n",
      "  <Module test_foo.py>\n",
      "    <Function test_success>\n",
      "    <Function test_fail>\n",
      "<Package subdir1>\n",
      "  <Module test_foo.py>\n",
      "    <Function test_success>\n",
      "    <Function test_fail>\n",
      "<Package subdir2>\n",
      "  <Module test_foo.py>\n",
      "    <Function test_success>\n",
      "    <Function test_fail>\n",
      "\n",
      "\u001b[32m========================== \u001b[32m6 tests collected\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ==========================\u001b[0m\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0\n",
      "rootdir: /home/vmonteco/dojo-pytest/tests/trucs/cache\n",
      "plugins: mock-3.12.0\n",
      "cachedir: /home/vmonteco/dojo-pytest/tests/trucs/cache/.pytest_cache\n",
      "----------------------------- cache values for '*' -----------------------------\n",
      "cache/stepwise contains:\n",
      "  []\n",
      "\n",
      "\u001b[33m============================ \u001b[33mno tests ran\u001b[0m\u001b[33m in 0.00s\u001b[0m\u001b[33m =============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "cd tests/trucs/cache\n",
    "# --cache-clear re-runs the tests without --collect-only :)\n",
    "pytest --cache-clear --collect-only\n",
    "pytest --cache-show\n",
    "cd ~/dojo-pytest/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0e4071",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MERCI !"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
